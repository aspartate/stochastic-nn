{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXxcASF4omQM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pickle, gzip\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import time\n",
        "import math\n",
        "import os\n",
        "from copy import deepcopy\n",
        "from scipy.special import expit # Vectorized sigmoid function\n",
        "from scipy.special import softmax as softmax_\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Function to show single image\n",
        "def show(image):\n",
        "    plt.figure(figsize=(10,10))\n",
        "    new = image.copy()\n",
        "    if image.shape == 3:                    # Switch R and B channels so it shows up as correctly as R,G,B, if image is 3-channel\n",
        "        new[:,:,0] = image[:,:,2]\n",
        "        new[:,:,2] = image[:,:,0]\n",
        "    plt.imshow(new, cmap = \"gray\")\n",
        "\n",
        "# Function to get adjacency matrix for a given latent distribution\n",
        "def get_adjacency_matrix(latent_distribution):\n",
        "    # Create dataframe\n",
        "    df = []\n",
        "    for index, element in enumerate(latent_distribution):\n",
        "        vec, label = element\n",
        "        df.append({'x': vec[0], 'y': vec[1], 'digit': label})\n",
        "    df = pd.DataFrame(df)\n",
        "\n",
        "    # Get centroid coordinates\n",
        "    centroids = df.groupby('digit').mean()\n",
        "\n",
        "    # Create adjacency matrix\n",
        "    adjacency_matrix = np.zeros((10, 10))\n",
        "    for i in range(10):\n",
        "        for j in range(10):\n",
        "            if i == j:\n",
        "                continue\n",
        "            else:\n",
        "                adjacency_matrix[i][j] = np.linalg.norm(centroids.iloc[i] - centroids.iloc[j]) # Euclidean distance (L2 norm)\n",
        "    \n",
        "    return adjacency_matrix\n",
        "\n",
        "# Function to compute adjacency spectral distance\n",
        "def adjacency_spectral_distance(adjacency_matrix_1, adjacency_matrix_2):\n",
        "    # Compute spectral distance\n",
        "    eigvals_1, eigvecs_1 = np.linalg.eig(adjacency_matrix_1)\n",
        "    eigvals_2, eigvecs_2 = np.linalg.eig(adjacency_matrix_2)\n",
        "    spectral_distance = np.linalg.norm(eigvals_1 - eigvals_2)\n",
        "\n",
        "    return spectral_distance\n",
        "\n",
        "# Function to compute cluster inertias\n",
        "def get_cluster_inertias(latent_distribution):\n",
        "    # Create dataframe\n",
        "    df = []\n",
        "    for index, element in enumerate(latent_distribution):\n",
        "        vec, label = element\n",
        "        df.append({'x': vec[0], 'y': vec[1], 'digit': label})\n",
        "    df = pd.DataFrame(df)\n",
        "\n",
        "    # Get centroid coordinates\n",
        "    centroids = df.groupby('digit').mean()\n",
        "\n",
        "    # Compute cluster inertias\n",
        "    cluster_inertias = []\n",
        "    for i in range(10):\n",
        "        centroid = centroids.iloc[i]\n",
        "        coords = df.loc[df['digit'] == i, ['x', 'y']].values\n",
        "        cluster_inertias.append(sum([(coord[0] - centroid['x'])**2 + (coord[1] - centroid['y'])**2 for coord in coords]))\n",
        "\n",
        "    return cluster_inertias # returns a list of cluster inertias, one for each digit\n",
        "\n",
        "# Prepare working directory\n",
        "for dirname in ['results/', 'results/p1', 'results/p2', 'results/p3']:\n",
        "    if os.path.isdir(dirname) == False:\n",
        "        os.mkdir(dirname)\n",
        "        print(f'{dirname} directory created.')\n",
        "    else:\n",
        "        print(f'{dirname} directory contains {len(os.listdir(dirname))} items.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPLucvB_omQR"
      },
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYYi8q5IomQT"
      },
      "outputs": [],
      "source": [
        "# Load MNIST data\n",
        "\n",
        "with gzip.open('data/mnist.pkl.gz', 'rb') as f:\n",
        "    train_data, val_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
        "    # train_data: tuple of (x_train, y_train), where x_train.shape = (50000, 784) and y_train.shape = (50000, 1)\n",
        "    # val_data: tuple of (x_val, y_val), where x_val.shape = (10000, 784) and y_val.shape = (10000, 1)\n",
        "    # test_data: tuple of (x_test, y_test), where x_test.shape = (10000, 784) and y_test.shape = (10000, 1)\n",
        "\n",
        "x_train, y_train = train_data\n",
        "x_val, y_val = val_data\n",
        "x_test, y_test = test_data\n",
        "\n",
        "# Combine training and validation data\n",
        "x_train = np.concatenate((x_train, x_val), axis=0)\n",
        "y_train = np.concatenate((y_train, y_val), axis=0)\n",
        "\n",
        "# Reshape x\n",
        "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train = np.eye(10)[y_train].reshape((-1, 10, 1))\n",
        "y_test = np.eye(10)[y_test].reshape((-1, 10, 1))\n",
        "\n",
        "# Zip data and labels into tuples\n",
        "train_data = list(zip(x_train, y_train))\n",
        "test_data = list(zip(x_test, y_test))\n",
        "\n",
        "# Shuffle training data\n",
        "np.random.shuffle(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6SfipNComQU"
      },
      "source": [
        "### Model constructor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kBU7iYnomQV"
      },
      "outputs": [],
      "source": [
        "# Constructor for neural network\n",
        "# Adapted from Nielsen textbook http://neuralnetworksanddeeplearning.com/chap1.html and https://towardsdatascience.com/mnist-handwritten-digits-classification-from-scratch-using-python-numpy-b08e401c4dab and https://github.com/geohot/ai-notebooks/blob/master/mnist_from_scratch.ipynb\n",
        "\n",
        "class Model(object):\n",
        "\n",
        "    # Fully connected neural network with layer i having sizes[i] neurons\n",
        "    def __init__(self, sizes, weights=None, biases=None):\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "\n",
        "        # Initialize weights & biases if not provided\n",
        "        if weights is None:\n",
        "            self.weights = [np.random.randn(y, x) for y, x in zip(sizes[1:], sizes[:-1])] # Initialize weights array of shape (num_neurons_i, num_neurons_i-1) for each layer i except the input layer\n",
        "        else:\n",
        "            self.weights = weights\n",
        "        \n",
        "        if biases is None:\n",
        "            self.biases = [np.random.randn(y, 1) for y in sizes[1:]] # Initialize biases array of shape (num_neurons_i, 1) for each layer i except the input layer\n",
        "        else:\n",
        "            self.biases = biases\n",
        "        \n",
        "        # Initialize training history\n",
        "        self.history = {'train': {'acc':[], 'loss':[]}, 'val': {'acc':[], 'loss':[]}, 'latent':[], 'weights':[], 'biases':[]}\n",
        "\n",
        "        # Number of parameters\n",
        "        print(f'Number of model parameters: {sum(np.prod(w.shape) for w in self.weights) + sum(np.prod(b.shape) for b in self.biases)}')\n",
        "\n",
        "    # Activation functions\n",
        "    # x is a matrix with nrows = num_hidden_neurons and ncols = batch_size generated by hidden layer affine transformation\n",
        "    def activation(self, x):\n",
        "        # Overflow-safe vectorized sigmoid function (https://stackoverflow.com/questions/51976461/optimal-way-of-defining-a-numerically-stable-sigmoid-function-for-a-list-in-pyth) \n",
        "        return np.where(x >= 0, \n",
        "                    1 / (1 + np.exp(-x)), \n",
        "                    np.exp(x) / (1 + np.exp(x)))\n",
        "\n",
        "    # Derivative of activation function\n",
        "    def activation_der(self, x):\n",
        "        return self.activation(x) * (1 - self.activation(x))\n",
        "\n",
        "    # Softmax function\n",
        "    # Overflow-safe softmax function (https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/)\n",
        "    def softmax(self, x):\n",
        "        # x is a matrix with nrows = num_hidden_neurons and ncols = batch_size generated by hidden layer affine transformation\n",
        "        # return softmax_(x, axis = 0)\n",
        "        return self.activation(x)\n",
        "\n",
        "    # Derivative of softmax function\n",
        "    def softmax_der(self, x):\n",
        "        return self.softmax(x) * (1 - self.softmax(x))        \n",
        "\n",
        "    # Loss function\n",
        "    def loss(self, y_true, y_pred):\n",
        "        return np.mean(np.sum(np.nan_to_num(-y_true*np.log(y_pred)-(1-y_true)*np.log(1-y_pred)), axis = 0)) # Cross entropy loss\n",
        "        # return np.mean(((y_true - y_pred)**2)) # mean squared error\n",
        "\n",
        "    # Derivative of loss function\n",
        "    def loss_der(self, y_true, y_pred):\n",
        "        return y_pred - y_true # Derivative of cross entropy loss\n",
        "        # return 2*(y_pred - y_true) # Derivative of mean squared error\n",
        "\n",
        "    # Backpropagation\n",
        "    # Input x,y are batch matrices with each column being the vector of a single training example\n",
        "    # Returns (nabla_b, nabla_w), where nabla_b is list of gradients of cost with respect to biases (one matrix for each layer) and nabla_w is list of gradients of cost with respect to weights (one matrix for each layer)\n",
        "    def backprop(self, x, y):\n",
        "        # Initialize lists of gradients for each layer\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        \n",
        "        ###### Forward pass, storing weighted inputs (z) and activations for each layer ######\n",
        "        activation = x # Initialize activation with input\n",
        "        activations = [x] # list to store all the activations, layer by layer\n",
        "        zs = [] # list to store all the weighted input z vectors, layer by layer\n",
        "        # Iterate through each layer\n",
        "        for i, tup in enumerate(zip(self.biases, self.weights)):\n",
        "            b, w = tup # Unpack biases and weights\n",
        "            z = np.dot(w, activation) + b # Compute weighted input z\n",
        "            zs.append(z) # Store weighted input z          \n",
        "            if i == (self.num_layers - 2):\n",
        "                # If last layer, apply softmax\n",
        "                activation = self.softmax(z)\n",
        "            else:\n",
        "                # If not last layer, apply activation function\n",
        "                activation = self.activation(z)\n",
        "            activations.append(activation) # Store activation\n",
        "\n",
        "        ###### Backward pass, computing gradients of cost with respect to biases and weights ######\n",
        "        # Get gradients for output layer\n",
        "        delta = self.loss_der(y, activations[-1]) * self.softmax_der(zs[-1]) # Hadamard product of loss gradient and softmax derivative for output layer\n",
        "        nabla_b[-1] = delta # Store gradient of cost with respect to biases of last layer\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose()) # Store gradient of cost with respect to weights of last layer\n",
        "        # Iterate through each layer in reverse order, starting from second to last layer\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l] # Retrieve weighted input z for current layer\n",
        "            delta = np.dot(self.weights[-l+1].transpose(), delta) * self.activation_der(z) # Compute gradient of cost with respect to weighted input z\n",
        "            nabla_b[-l] = delta # Store gradient of cost with respect to biases of current layer\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose()) # Store gradient of cost with respect to weights of current layer\n",
        "\n",
        "        # Sum columns of nabla_b matrices to get overall bias gradients for batch\n",
        "        nabla_b = [np.sum(nabla_b[i], axis=1).reshape(nabla_b[i].shape[0], 1) for i in range(len(nabla_b))]\n",
        "\n",
        "        # Get training performance for batch\n",
        "        acc = np.mean(np.argmax(activations[-1], axis=0) == np.argmax(y, axis=0)) # For each column of activations[-1], get predicted label and compare to true label\n",
        "        loss = self.loss(y, activations[-1]) # Compute loss\n",
        "        \n",
        "        return nabla_b, nabla_w, acc, loss\n",
        "\n",
        "    # Model training using SGD\n",
        "    # training_data is a list of tuples (x, y) representing the training inputs and the desired outputs\n",
        "    def fit(self, training_data, epochs, batch_size, learning_rate, decay_strength, validation_size, shuffle_order, store_latent_vecs, verbose=2):\n",
        "        \n",
        "        # Take last portion of training data as validation data\n",
        "        training_data = training_data[:-int(validation_size*len(training_data))]\n",
        "        val_data = training_data[-int(validation_size*len(training_data)):]\n",
        "\n",
        "        x_train = np.array([x for x, y in training_data])[:,:,0].T # Retrieve matrix of training examples where each column is a training example\n",
        "        y_train = np.array([y for x, y in training_data])[:,:,0].T # Retrieve matrix of training labels where each column is the one-hot encoded vector\n",
        "\n",
        "        ## Store baseline model characteristics (before training)\n",
        "        # Evaluate model on training data\n",
        "        train_acc, train_loss = self.evaluate(training_data)\n",
        "        # Evaluate model on validation data\n",
        "        val_acc, val_loss = self.evaluate(val_data)\n",
        "        print(f\"Baseline characteristics: Training accuracy: {train_acc:.4f}. Validation accuracy: {val_acc:.4f}. Training loss: {train_loss:.4f}. Validation loss: {val_loss:.4f}\")\n",
        "        # Update global history\n",
        "        self.history['train']['acc'].append(train_acc)\n",
        "        self.history['train']['loss'].append(train_loss)\n",
        "        self.history['val']['acc'].append(val_acc)\n",
        "        self.history['val']['loss'].append(val_loss)\n",
        "        # Store latent vectors\n",
        "        if store_latent_vecs:\n",
        "            vecs = self.feedforward(x_train[:, :10000], layer_num = 2).T\n",
        "            labs = np.argmax(y_train[:, :10000], axis=0)\n",
        "            self.history['latent'].append(list(zip(vecs, labs))) # Store latent representation of training data before training\n",
        "        # Store layer weights\n",
        "        self.history['weights'].append(self.weights)\n",
        "        # Store layer biases\n",
        "        self.history['biases'].append(self.biases)\n",
        "\n",
        "        # Iterate through each epoch\n",
        "        print(f'Training on {len(training_data)} examples, validating on {len(val_data)} examples')\n",
        "        for j in range(epochs):\n",
        "            # Initialize epoch training accuracy and loss history\n",
        "            train_acc = []\n",
        "            train_loss = []\n",
        "            # Get time\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Adjust learning rate for current epoch based on decay strength\n",
        "            learning_rate_adj = learning_rate/(1 + decay_strength * j)\n",
        "\n",
        "            # Shuffle training data in preparation for SGD batching\n",
        "            if shuffle_order is None:\n",
        "                np.random.shuffle(training_data) # Shuffle training data randomly\n",
        "            else:\n",
        "                training_data = np.array(training_data, dtype=object)[shuffle_order[j]] # Shuffle training data according to order provided\n",
        "            # Construct training batches for SGD\n",
        "            batches = [training_data[k:k+batch_size] for k in range(0, len(training_data), batch_size)]\n",
        "            # Iterate through all batches\n",
        "            for batch in batches:\n",
        "                x_batch = np.array([x for x, y in batch])[:,:,0].T # Retrieve matrix of training examples where each column is a training example\n",
        "                y_batch = np.array([y for x, y in batch])[:,:,0].T # Retrieve matrix of training labels where each column is the one-hot encoded vector\n",
        "\n",
        "                nabla_b_, nabla_w_, acc, loss = self.backprop(x_batch, y_batch) # Compute loss gradients for each layer for whole batch\n",
        "\n",
        "                # Update epoch training accuracy and loss history for whole batch\n",
        "                train_acc.append(acc)\n",
        "                train_loss.append(loss)\n",
        "\n",
        "                # Update weights and biases for each layer using average loss gradients\n",
        "                self.weights = [w-(learning_rate_adj*nw) for w, nw in zip(self.weights, nabla_w_)] # Need to divide by batch size to get average gradient per example\n",
        "                self.biases = [b-(learning_rate_adj*nb) for b, nb in zip(self.biases, nabla_b_)] # Need to divide by batch size to get average gradient per example\n",
        "            \n",
        "            # Compute epoch training accuracy and loss\n",
        "            train_acc, train_loss = np.mean(train_acc), np.mean(train_loss)\n",
        "            # Evaluate model on validation data\n",
        "            val_acc, val_loss = self.evaluate(val_data)\n",
        "            # Get time\n",
        "            end_time = time.time()\n",
        "            # Print epoch number (epochs start at 1)\n",
        "            if verbose >= 2:\n",
        "                print(f\"Epoch {j+1} complete. Time taken: {end_time - start_time:.2f} seconds.\")\n",
        "            if verbose >= 1:\n",
        "                print(f\"Learning rate: {learning_rate_adj:.4f}. Training accuracy: {train_acc:.4f}. Validation accuracy: {val_acc:.4f}. Mean training loss: {train_loss:.4f}. Mean validation loss: {val_loss:.4f}. Mean weight gradients: {[np.around(np.mean(np.abs(nw)), 4) for nw in nabla_w_]}.\")\n",
        "\n",
        "            # Update global history\n",
        "            self.history['train']['acc'].append(train_acc)\n",
        "            self.history['train']['loss'].append(train_loss)\n",
        "            self.history['val']['acc'].append(val_acc)\n",
        "            self.history['val']['loss'].append(val_loss)\n",
        "            # Store latent vectors\n",
        "            if store_latent_vecs:\n",
        "                vecs = self.feedforward(x_train[:, :10000], layer_num = 2).T.round(2)\n",
        "                labs = np.argmax(y_train[:, :10000], axis=0)\n",
        "                self.history['latent'].append(list(zip(vecs, labs))) # Store latent representation of training data before training\n",
        "            # Store layer weights\n",
        "            self.history['weights'].append(self.weights)\n",
        "            # Store layer biases\n",
        "            self.history['biases'].append(self.biases)\n",
        "\n",
        "        print(\"Training complete.\")\n",
        "\n",
        "    def evaluate(self, test_data):\n",
        "        x_test = np.array([x for x, y in test_data])[:,:,0].T # Retrieve matrix of training examples where each column is a training example\n",
        "        y_test = np.array([y for x, y in test_data])[:,:,0].T # Retrieve matrix of training labels where each column is the one-hot encoded vector\n",
        "\n",
        "        acc = np.mean(np.argmax(self.feedforward(x_test), axis=0) == np.argmax(y_test, axis=0))\n",
        "        loss = self.loss(y_test, self.feedforward(x_test)) \n",
        "\n",
        "        return acc, loss\n",
        "\n",
        "    # Get feedforward activations for given layer (default is last layer)\n",
        "    # Equivalent to Tensorflow get_layer()\n",
        "    def feedforward(self, x, layer_num = None):\n",
        "        # Dynamically update activations for each layer while moving forward through the network, starting with shape (input_size, 1) and ending with shape (output_size, 1)\n",
        "        if layer_num is None:\n",
        "            layer_num = self.num_layers - 1\n",
        "        activations = x\n",
        "        current_layer = 1\n",
        "        while current_layer <= layer_num:\n",
        "            # If current layer is last layer, return softmax activations\n",
        "            if current_layer == self.num_layers - 1:\n",
        "                activations = self.softmax(np.dot(self.weights[current_layer - 1], activations) + self.biases[current_layer - 1])\n",
        "            else:\n",
        "                activations = self.activation(np.dot(self.weights[current_layer - 1], activations) + self.biases[current_layer - 1])\n",
        "            current_layer += 1\n",
        "        return activations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkguy2EaomQb"
      },
      "source": [
        "### Set global params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAvpND4WomQd"
      },
      "outputs": [],
      "source": [
        "# Model architecture\n",
        "sizes = [784, 30, 2, 10]\n",
        "epochs = 99\n",
        "batch_size = 1000\n",
        "learning_rate = 0.01\n",
        "decay_strength = 0\n",
        "validation_size = 0.1\n",
        "\n",
        "# Set seed for reproducibility\n",
        "np.random.seed(1)\n",
        "\n",
        "# Initialize weights array of shape (num_neurons_i, num_neurons_i-1) for each layer i except the input layer\n",
        "weights = [np.random.randn(y, x) for y, x in zip(sizes[1:], sizes[:-1])]\n",
        "\n",
        "# Initialize biases array of shape (num_neurons_i, 1) for each layer i except the input layer\n",
        "biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "\n",
        "# Initialize list of data indices for data shuffling per epoch\n",
        "shuffle_order = [np.random.permutation(int(len(train_data)*(1 - validation_size))) for i in range(epochs)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jF6qipn8omQe"
      },
      "source": [
        "### Verify that model training is reproducible"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WvYJXv8omQf"
      },
      "outputs": [],
      "source": [
        "# # Build & train first model\n",
        "# model1 = Model(sizes, weights, biases)\n",
        "# model1.fit(train_data, epochs = epochs, batch_size = batch_size, learning_rate = learning_rate, decay_strength = decay_strength, validation_size = validation_size, shuffle_order = shuffle_order, store_latent_vecs = False)\n",
        "# test_acc, test_loss = model1.evaluate(test_data)\n",
        "# print(f'Test accuracy: {test_acc}. Mean test loss: {test_loss}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KblHjCvNomQf"
      },
      "outputs": [],
      "source": [
        "# # Build & train second model\n",
        "# model2 = Model(sizes, weights, biases)\n",
        "# model2.fit(train_data, epochs = epochs, batch_size = batch_size, learning_rate = learning_rate, decay_strength = decay_strength, validation_size = validation_size, shuffle_order = shuffle_order, store_latent_vecs = False)\n",
        "# test_acc, test_loss = model2.evaluate(test_data)\n",
        "# print(f'Test accuracy: {test_acc}. Mean test loss: {test_loss}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZuFWSKtnomQg"
      },
      "outputs": [],
      "source": [
        "# # Are model weights reproducible?\n",
        "# print(f'Model weights were reproducible: {[np.array_equal(model1.weights[i], model2.weights[i]) for i in range(len(model1.weights))]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9q5WQehAomQg"
      },
      "outputs": [],
      "source": [
        "# # Inspect last layer activations\n",
        "# last_layer = [model1.feedforward(x, layer_num = 3) for x, y in train_data[:1]]\n",
        "# print(last_layer[0].sum())\n",
        "# print(last_layer[0])\n",
        "# print(train_data[0][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXfAMyWcomQg"
      },
      "source": [
        "## PART 1. Visualize latent space over epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpDLCrjPomQh",
        "outputId": "249c833f-5e7d-4232-d3db-01ecbf86c3ff"
      },
      "outputs": [],
      "source": [
        "path = 'results/p1/model.pkl.gz'\n",
        "if not os.path.exists(path):\n",
        "    # Build & train model\n",
        "    model = Model(sizes, weights, biases)\n",
        "    model.fit(train_data, epochs = epochs, batch_size = batch_size, learning_rate = learning_rate, decay_strength = decay_strength, validation_size = validation_size, shuffle_order = shuffle_order, store_latent_vecs = True)\n",
        "    test_acc, test_loss = model.evaluate(test_data)\n",
        "    print(f'Test accuracy: {test_acc}. Mean test loss: {test_loss}')\n",
        "\n",
        "    # Save model to file\n",
        "    print(\"Saving model to file...\")\n",
        "    start = time.time()\n",
        "    with gzip.open(path, 'wb') as f:\n",
        "        pickle.dump(model, f)\n",
        "    print(f\"Model saved in {time.time() - start} seconds.\")\n",
        "else:\n",
        "    # Read model from file\n",
        "    with gzip.open(path, 'rb') as f:\n",
        "        model = pickle.load(f)\n",
        "    print(f'Model loaded from file.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "NNQJ_ZvsomQi",
        "outputId": "676aa0b7-2ddc-473b-affb-66242c1382e8"
      },
      "outputs": [],
      "source": [
        "path = 'results/p1/history.png'\n",
        "if not os.path.exists(path):\n",
        "    # Plot model history\n",
        "    fig, ax = plt.subplots(1, 2, figsize = (14, 5))\n",
        "\n",
        "    # Plot accuracy history\n",
        "    ax[0].plot(range(len(model.history['val']['acc'])), model.history['val']['acc'], label='Validation')\n",
        "    ax[0].plot(range(len(model.history['train']['acc'])), model.history['train']['acc'], label='Train')\n",
        "    ax[0].set_xticks(np.arange(0, len(model.history['val']['acc']), 10)) # Set xticks\n",
        "    ax[0].set_ylabel('Accuracy') # Y axis label\n",
        "    ax[0].set_xlabel('Epochs') # X axis label\n",
        "    ax[0].legend() # Add legend\n",
        "\n",
        "    # Plot loss history\n",
        "    ax[1].plot(range(len(model.history['val']['loss'])), model.history['val']['loss'], label='Validation')\n",
        "    ax[1].plot(range(len(model.history['train']['loss'])), model.history['train']['loss'], label='Train')\n",
        "    ax[1].set_xticks(np.arange(0, len(model.history['val']['loss']), 10)) # Set xticks\n",
        "    ax[1].set_ylabel('Loss') # Y axis label\n",
        "    ax[1].set_xlabel('Epochs') # X axis label\n",
        "    ax[1].legend() # Add legend\n",
        "\n",
        "    # Save plot to file\n",
        "    plt.savefig(path)\n",
        "    plt.show();\n",
        "else:\n",
        "    # Load plot from file\n",
        "    print('Loading image from file.')\n",
        "    plt.figure(figsize = (14, 5))\n",
        "    plt.axes([0,0,1,1])\n",
        "    plt.imshow(mpimg.imread(path))\n",
        "    plt.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giGS_ASMomQj"
      },
      "outputs": [],
      "source": [
        "path = 'results/p1/latent.png'\n",
        "if not os.path.exists(path):\n",
        "    # Retrieve latent vectors from model history and plot them\n",
        "    nrows = math.ceil(epochs/10)\n",
        "    ncols = 10\n",
        "    fig, ax = plt.subplots(nrows, ncols, tight_layout=True, figsize = (ncols * 5, nrows * 5))\n",
        "    ax = ax.flatten()\n",
        "\n",
        "    colors = sns.color_palette(['#e6194B','#f58231','#ffe119','#bfef45','#3cb44b','#42d4f4','#4363d8', '#000075', '#911eb4','#f032e6'])\n",
        "\n",
        "    for epoch in range(0, len(model.history['latent'])):\n",
        "        # Create dataframe\n",
        "        df = []\n",
        "        for index, element in enumerate(model.history['latent'][epoch][:10000]): # Limit to first 10000 examples\n",
        "            vec, label = element\n",
        "            df.append({'x': vec[0], 'y': vec[1], 'digit': label})\n",
        "        df = pd.DataFrame(df)\n",
        "\n",
        "        # Plot latent vectors in 2D, color-coded by digit\n",
        "        # print(f'Plotting {len(df)} examples.')\n",
        "        sns.scatterplot(data=df, x=\"x\", y=\"y\", hue=\"digit\", palette=colors, ax=ax[epoch], legend=False)\n",
        "\n",
        "        # Add title\n",
        "        if epoch == 0:\n",
        "            title = 'Before training'\n",
        "        else:\n",
        "            title = f'After epoch {epoch}'\n",
        "        ax[epoch].set_title(title)\n",
        "\n",
        "    # Create legend in last subplot\n",
        "    for i in range(10):\n",
        "        ax[-1].scatter([], [], color=colors[i], label=i)\n",
        "    ax[-1].legend()\n",
        "\n",
        "    # Save plot to file\n",
        "    plt.savefig(path)\n",
        "    plt.show();\n",
        "else:\n",
        "    # Load plot from file\n",
        "    print('Loading image from file.')\n",
        "    plt.figure(figsize = (35, 35))\n",
        "    plt.axes([0,0,1,1])\n",
        "    plt.imshow(mpimg.imread(path))\n",
        "    plt.axis('off')\n",
        "    plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quantify cluster cohesion (inertia)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot inertia of each cluster over epochs\n",
        "path = 'results/p1/inertia.png'\n",
        "if not os.path.exists(path):\n",
        "    colors = sns.color_palette(['#e6194B','#f58231','#ffe119','#bfef45','#3cb44b','#42d4f4','#4363d8', '#000075', '#911eb4','#f032e6'])\n",
        "\n",
        "    # Get list of cluster inertias for each epoch\n",
        "    cluster_inertias = []\n",
        "    for epoch in range(len(model.history['latent'])):\n",
        "        cluster_inertias.append(get_cluster_inertias(model.history['latent'][epoch]))\n",
        "    \n",
        "    # Convert to numpy array and transpose\n",
        "    cluster_inertias = np.array(cluster_inertias).T\n",
        "\n",
        "    # Plot each row of cluster inertias in different colors\n",
        "    for i in range(10):\n",
        "        plt.plot(cluster_inertias[i], color=colors[i], label=i)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Cluster inertia')\n",
        "    plt.title(f'train_acc: {model.history[\"train\"][\"acc\"][-1]:.2f}')\n",
        "    plt.legend()\n",
        "\n",
        "    # Save plot to file\n",
        "    plt.savefig(path);\n",
        "    plt.show();\n",
        "else:\n",
        "    # Load plot from file\n",
        "    print('Loading image from file.')\n",
        "    plt.axes([0,0,1,1])\n",
        "    plt.imshow(mpimg.imread(path))\n",
        "    plt.axis('off')\n",
        "    plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quantify cluster separation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEO5epYTomQk"
      },
      "outputs": [],
      "source": [
        "path = 'results/p1/cluster_sep.png'\n",
        "if not os.path.exists(path):\n",
        "    # Plot separation between clusters over epochs for all pairs of digits\n",
        "    nrows = 2\n",
        "    ncols = 5\n",
        "    fig, ax = plt.subplots(nrows, ncols, tight_layout=True, figsize = (ncols * 5, nrows * 5))\n",
        "    ax = ax.flatten()\n",
        "\n",
        "    colors = sns.color_palette(['#e6194B','#f58231','#ffe119','#bfef45','#3cb44b','#42d4f4','#4363d8', '#000075', '#911eb4','#f032e6'])\n",
        "\n",
        "    for digit1 in range(10):\n",
        "        dict_of_distances = {}\n",
        "        for digit2 in range(10):\n",
        "            distances = []\n",
        "            for epoch in range(0, len(model.history['latent'])):\n",
        "                # Get separation between digit1 and digit2 cluster centroids from adjacency matrix\n",
        "                distances.append(get_adjacency_matrix(model.history['latent'][epoch])[digit1][digit2])\n",
        "            dict_of_distances[digit2] = distances\n",
        "        df = pd.DataFrame(dict_of_distances)\n",
        "\n",
        "        # Plot separation between clusters over epochs for all pairs of digits\n",
        "        sns.lineplot(data=df, ax=ax[digit1], legend=True)\n",
        "        ax[digit1].set_title(f'Separation from digit {digit1} over epochs')\n",
        "        ax[digit1].set_xlabel('Epoch')\n",
        "        ax[digit1].set_ylabel('Separation')\n",
        "        ax[digit1].set_xticks(np.arange(0, len(model.history['val']['loss']), 10)) # Set xticks\n",
        "    \n",
        "    # Add title\n",
        "    title = 'Separation between clusters over epochs for all pairs of digits'\n",
        "    fig.suptitle(title)\n",
        "\n",
        "    # Save plot to file\n",
        "    plt.savefig(path)\n",
        "    plt.show();\n",
        "else:\n",
        "    # Load plot from file\n",
        "    print('Loading image from file.')\n",
        "    plt.axes([0,0,1,1])\n",
        "    plt.imshow(mpimg.imread(path))\n",
        "    plt.axis('off')\n",
        "    plt.show();            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCYNBrDuomQj"
      },
      "source": [
        "### Quantify cluster separation vs. training loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAqZ-ql6omQj"
      },
      "outputs": [],
      "source": [
        "path = 'results/p1/adjacency_mean_v_loss.png'\n",
        "if not os.path.exists(path):\n",
        "    # Mean of adjacency matrix vs. training loss\n",
        "    # Calculate adjacency matrix mean for each epoch, including before training\n",
        "    adjacency_matrix_means = []\n",
        "    for epoch in range(0, len(model.history['latent'])):\n",
        "        adjacency_matrix_means.append(np.mean(get_adjacency_matrix(model.history['latent'][epoch])))\n",
        "\n",
        "    # Get list of training loss values\n",
        "    train_losses = model.history['train']['loss']\n",
        "\n",
        "    # Plot adjacency mean vs. training loss\n",
        "    # Color by epoch\n",
        "    plt.scatter(adjacency_matrix_means, model.history['train']['loss'], c=range(len(model.history['train']['loss'])))\n",
        "    plt.xlabel('Adjacency mean')\n",
        "    plt.ylabel('Training loss')\n",
        "    plt.title('Adjacency mean vs. training loss')\n",
        "\n",
        "    # Save plot to file\n",
        "    plt.savefig(path)\n",
        "    plt.show();\n",
        "else:\n",
        "    # Load plot from file\n",
        "    print('Loading image from file.')\n",
        "    plt.axes([0,0,1,1])\n",
        "    plt.imshow(mpimg.imread(path))\n",
        "    plt.axis('off')\n",
        "    plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quantify cLGG similarity vs. performance similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QcHzr5HomQk"
      },
      "outputs": [],
      "source": [
        "path = 'results/p1/spectral_dist_v_loss_delta.png'\n",
        "if not os.path.exists(path):\n",
        "    # Adjacency spectral distance vs. change in training loss for consecutive epochs\n",
        "    # Calculate adjacency spectral distance between latent distributions of consecutive epochs (starting from epoch 1)\n",
        "    spectral_distances = []\n",
        "    for epoch in range(0, len(model.history['latent']) - 1):\n",
        "        adjacency_matrix_1 = get_adjacency_matrix(model.history['latent'][epoch])\n",
        "        adjacency_matrix_2 = get_adjacency_matrix(model.history['latent'][epoch + 1])\n",
        "        spectral_distances.append(adjacency_spectral_distance(adjacency_matrix_1, adjacency_matrix_2))\n",
        "\n",
        "    # Get change in training loss between consecutive epochs (starting from epoch 0)\n",
        "    loss_deltas = []\n",
        "    for epoch in range(0, len(model.history['train']['loss']) - 1):\n",
        "        loss_deltas.append(np.abs(model.history['train']['loss'][epoch + 1] - model.history['train']['loss'][epoch]))\n",
        "\n",
        "    # Plot spectral distance vs. change in training loss\n",
        "    # Color by epoch\n",
        "    plt.scatter(spectral_distances, loss_deltas, c=range(0, len(model.history['train']['loss']) - 1))\n",
        "    plt.xlabel('Spectral distance')\n",
        "    plt.ylabel('Change in training loss')\n",
        "    plt.title('Spectral distance vs. change in training loss')\n",
        "\n",
        "    # Save plot to file\n",
        "    plt.savefig(path, bbox_inches='tight')\n",
        "    plt.show();\n",
        "else:\n",
        "    # Load plot from file\n",
        "    print('Loading image from file.')\n",
        "    plt.axes([0,0,1,1])\n",
        "    plt.imshow(mpimg.imread(path))\n",
        "    plt.axis('off')\n",
        "    plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compare change in cLGG vs. change in model weights across epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Sq5cYTbomQl"
      },
      "outputs": [],
      "source": [
        "path = 'results/p1/spectral_dist_v_weights_dist.png'\n",
        "if not os.path.exists(path):\n",
        "    # Adjacency spectral distance vs. Frobenius weights distance for consecutive epochs\n",
        "    # Get Frobenius distance between bottleneck weight matrices of consecutive epochs\n",
        "    weight_distances = []\n",
        "    for i in range(0, len(model.history['weights']) - 1):\n",
        "        weight_distances.append(np.linalg.norm(model.history['weights'][i + 1][-2] - model.history['weights'][i][-2])) # Bottleneck layer is second to last layer\n",
        "\n",
        "    print(len(model.history['weights']), len(model.history['latent']))\n",
        "    # Plot spectral distance vs. Frobenius weights distance\n",
        "    plt.scatter(spectral_distances, weight_distances, c=range(0, len(model.history['train']['loss']) - 1))\n",
        "    plt.xlabel('Spectral distance')\n",
        "    plt.ylabel('Frobenius weights distance')\n",
        "    plt.title('Spectral distance vs. Frobenius weights distance')\n",
        "\n",
        "    # Save plot to file\n",
        "    plt.savefig(path)\n",
        "    plt.show();\n",
        "else:\n",
        "    # Load plot from file\n",
        "    print('Loading image from file.')\n",
        "    plt.axes([0,0,1,1])\n",
        "    plt.imshow(mpimg.imread(path))\n",
        "    plt.axis('off')\n",
        "    plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRktrzdgomQq"
      },
      "source": [
        "## Part 2. Full random search of initial weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBBY-9nJomQq"
      },
      "outputs": [],
      "source": [
        "num_models = 100\n",
        "path = 'results/p2'\n",
        "\n",
        "# Set seed for reproducibility\n",
        "np.random.seed(1)\n",
        "\n",
        "# Initialize list of lists weights array of shape (num_neurons_i, num_neurons_i-1) for each layer i except the input layer\n",
        "weights_list = [[np.random.randn(y, x) for y, x in zip(sizes[1:], sizes[:-1])] for _ in range(num_models)]\n",
        "\n",
        "# Initialize biases array of shape (num_neurons_i, 1) for each layer i except the input layer\n",
        "biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "\n",
        "# Initialize list of data indices for data shuffling per epoch\n",
        "shuffle_order = [np.random.permutation(int(len(train_data)*(1 - validation_size))) for i in range(epochs)]\n",
        "\n",
        "# Train 100 models with different initial weights\n",
        "models_list = []\n",
        "for idx, i in enumerate(range(num_models)):\n",
        "    if not os.path.exists(f'{path}/model_{idx}.pkl.gz'):\n",
        "        weights = weights_list[idx]\n",
        "        \n",
        "        # Build & train model\n",
        "        print(f'Training model {idx}...')\n",
        "        model = Model(sizes, weights, biases)\n",
        "        model.fit(train_data, epochs = epochs, batch_size = batch_size, learning_rate = learning_rate, decay_strength = decay_strength, validation_size = validation_size, shuffle_order = shuffle_order, store_latent_vecs = True, verbose=0)\n",
        "        models_list.append(deepcopy(model)) # Deepcopy to avoid overwriting previous models\n",
        "\n",
        "        # Save model to file\n",
        "        print(f'Saving model {idx} to file...')\n",
        "        start_time = time.time()\n",
        "        with gzip.open(f'{path}/model_{idx}.pkl.gz', 'wb') as f:\n",
        "            pickle.dump(model, f)\n",
        "        print(f'Done. Saving took {time.time() - start_time:.2f} seconds.')\n",
        "    else:\n",
        "        # Open model from file\n",
        "        print(f'Opening model {idx} from file...')\n",
        "        start_time = time.time()\n",
        "        with gzip.open(f'{path}/model_{idx}.pkl.gz', 'rb') as f:\n",
        "            models_list.append(pickle.load(f))\n",
        "        print(f'Done. Opening took {time.time() - start_time:.2f} seconds.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZxkXsFKomQq"
      },
      "outputs": [],
      "source": [
        "path = 'results/p2/acc.png'\n",
        "if not os.path.exists(path):\n",
        "    # Plot histogram of final training accuracy\n",
        "    plt.hist([model.history['train']['acc'][-1] for model in models_list], bins=10)\n",
        "    plt.xlabel('Final training accuracy')\n",
        "    plt.ylabel('Number of models')\n",
        "    plt.title(f'Variation in training accuracy after {len(models_list[0].history[\"train\"][\"loss\"]) - 1} epochs')\n",
        "    plt.savefig(path);\n",
        "    plt.show();\n",
        "else:\n",
        "    # Load plot from file\n",
        "    print('Loading image from file.')\n",
        "    plt.axes([0,0,1,1])\n",
        "    plt.imshow(mpimg.imread(path))\n",
        "    plt.axis('off')\n",
        "    plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lfyk738omQr"
      },
      "outputs": [],
      "source": [
        "path = 'results/p2/history.png'\n",
        "if not os.path.exists(path):\n",
        "    # Plot training history for each model\n",
        "    nrows = 10\n",
        "    ncols = 10\n",
        "    fig, ax = plt.subplots(nrows, ncols, tight_layout=True, figsize = (ncols * 5, nrows * 5))\n",
        "    ax = ax.flatten()\n",
        "\n",
        "    for i, model in enumerate(models_list):\n",
        "        # Plot accuracy history\n",
        "        ax[i].plot(range(len(model.history['val']['acc'])), model.history['val']['acc'], label='Validation')\n",
        "        ax[i].plot(range(len(model.history['train']['acc'])), model.history['train']['acc'], label='Train')\n",
        "        ax[i].set_xticks(np.arange(0, len(model.history['val']['acc']), 10)) # Set xticks\n",
        "        ax[i].set_ylabel('Accuracy') # Y axis label\n",
        "        ax[i].set_xlabel('Epochs') # X axis label\n",
        "        ax[i].legend() # Add legend\n",
        "\n",
        "        # Add title\n",
        "        title = f'val_acc: {model.history[\"val\"][\"acc\"][-1]:.2f}'\n",
        "        ax[i].set_title(title)\n",
        "\n",
        "    # Save plot to file\n",
        "    plt.savefig(path);\n",
        "    plt.show();\n",
        "else:\n",
        "    # Load plot from file\n",
        "    print('Loading image from file.')\n",
        "    plt.axes([0,0,1,1])\n",
        "    plt.imshow(mpimg.imread(path))\n",
        "    plt.axis('off')\n",
        "    plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akWJdkwRomQr"
      },
      "outputs": [],
      "source": [
        "path = 'results/p2/latent.png'\n",
        "if not os.path.exists(path):\n",
        "    # Visualize final latent space for each model\n",
        "    nrows = 10\n",
        "    ncols = 10\n",
        "    fig, ax = plt.subplots(nrows, ncols, tight_layout=True, figsize = (ncols * 5, nrows * 5))\n",
        "    ax = ax.flatten()\n",
        "\n",
        "    colors = sns.color_palette(['#e6194B','#f58231','#ffe119','#bfef45','#3cb44b','#42d4f4','#4363d8', '#000075', '#911eb4','#f032e6'])\n",
        "\n",
        "    for i, model in enumerate(models_list):\n",
        "        # Get latent vecs for final epoch\n",
        "        latent_vecs = model.history['latent'][-1]\n",
        "        # Create dataframe\n",
        "        df = []\n",
        "        for index, element in enumerate(latent_vecs[:10000]): # Limit to first 10000 example\n",
        "            vec, label = element\n",
        "            df.append({'x': vec[0], 'y': vec[1], 'digit': label})\n",
        "        df = pd.DataFrame(df)\n",
        "\n",
        "        # Plot latent vectors in 2D, color-coded by digit\n",
        "        # print(f'Plotting {len(df)} examples.')\n",
        "        sns.scatterplot(data=df, x=\"x\", y=\"y\", hue=\"digit\", palette=colors, ax=ax[i], legend=False)\n",
        "\n",
        "        # Add title\n",
        "        title = f'weight[2][0][0] = {model.history[\"weights\"][0][2][0][0]:.3f}; val_acc: {model.history[\"val\"][\"acc\"][-1]:.2f}'\n",
        "        ax[i].set_title(title)\n",
        "\n",
        "    # Create legend in last subplot\n",
        "    for i in range(10):\n",
        "        ax[-1].scatter([], [], color=colors[i], label=i)\n",
        "    ax[-1].legend()\n",
        "\n",
        "    # Save plot to file\n",
        "    plt.savefig(path);\n",
        "    plt.show();\n",
        "else:\n",
        "    # Load plot from file\n",
        "    print('Loading image from file.')\n",
        "    plt.axes([0,0,1,1])\n",
        "    plt.imshow(mpimg.imread(path))\n",
        "    plt.axis('off')\n",
        "    plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXZ5YosLomQs"
      },
      "outputs": [],
      "source": [
        "path = 'results/p2/initweights_dist_v_cLGG_dist.png'\n",
        "if not os.path.exists(path):\n",
        "    # Plot Frobenius distance of first hidden layer initial weights versus spectral distance for each pair of models\n",
        "    frobenius_dists = []\n",
        "    spectral_dists = []\n",
        "    for i, model1 in enumerate(models_list):\n",
        "        for j, model2 in enumerate(models_list):\n",
        "            if i != j:\n",
        "                # Get first hidden layer initial weights for each model\n",
        "                init_weights_i = model1.history[\"weights\"][0][0]\n",
        "                init_weights_j = model2.history[\"weights\"][0][0]\n",
        "                # Get Frobenius distance\n",
        "                frobenius_dists.append(np.linalg.norm(init_weights_i - init_weights_j))\n",
        "                # Get spectral distance of last epoch latent vectors\n",
        "                adjacency_matrix_1 = get_adjacency_matrix(model1.history['latent'][-1])\n",
        "                adjacency_matrix_2 = get_adjacency_matrix(model2.history['latent'][-1])\n",
        "                spectral_dists.append(adjacency_spectral_distance(adjacency_matrix_1, adjacency_matrix_2))\n",
        "\n",
        "    # Plot Frobenius distance vs. spectral distance\n",
        "    plt.scatter(spectral_dists, frobenius_dists, s=2)\n",
        "    plt.xlabel('Spectral distance')\n",
        "    plt.ylabel('Frobenius distance')\n",
        "    plt.title(f'Frobenius distance of first hidden layer initial weights vs. spectral distance after {len(model1.history[\"train\"][\"loss\"]) - 1} epochs')\n",
        "\n",
        "    # Save plot to file\n",
        "    plt.savefig(path);\n",
        "    plt.show();\n",
        "else:\n",
        "    # Load plot from file\n",
        "    print('Loading image from file.')\n",
        "    plt.axes([0,0,1,1])\n",
        "    plt.imshow(mpimg.imread(path))\n",
        "    plt.axis('off')\n",
        "    plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkPCqZ_momQs"
      },
      "outputs": [],
      "source": [
        "path = 'results/p2/adjacency_mean_v_loss.png'\n",
        "if not os.path.exists(path):\n",
        "    # Plot mean adjacency versus final training loss for each model in models_list\n",
        "    \n",
        "    # Calculate mean adjacency for each model\n",
        "    adjacency_means = []\n",
        "    for model in models_list:\n",
        "        adjacency_means.append(np.mean(get_adjacency_matrix(model.history['latent'][-1])))\n",
        "    \n",
        "    # Get final training loss for each model\n",
        "    final_losses = []\n",
        "    for model in models_list:\n",
        "        final_losses.append(model.history['train']['loss'][-1])\n",
        "\n",
        "    # Plot mean adjacency versus final training loss\n",
        "    plt.scatter(adjacency_means, final_losses)\n",
        "    plt.xlabel('Adjacency mean')\n",
        "    plt.ylabel('Final training loss')\n",
        "    plt.title('Mean adjacency versus final training loss')\n",
        "\n",
        "    # Save plot to file\n",
        "    plt.savefig(path, bbox_inches='tight')\n",
        "    plt.show();\n",
        "else:\n",
        "    # Load plot from file\n",
        "    print('Loading image from file.')\n",
        "    plt.axes([0,0,1,1])\n",
        "    plt.imshow(mpimg.imread(path))\n",
        "    plt.axis('off')\n",
        "    plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quantify cLGG similarity vs. performance similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcQieoyIomQt"
      },
      "outputs": [],
      "source": [
        "path = 'results/p2/cLGG_dist_v_loss_delta.png'\n",
        "if not os.path.exists(path):\n",
        "    # Plot spectral distance versus loss difference for each pair of models\n",
        "    spectral_dists = []\n",
        "    loss_diffs = []\n",
        "    for i, model1 in enumerate(models_list):\n",
        "        for j, model2 in enumerate(models_list):\n",
        "            if i < j:\n",
        "                # Get spectral distance of last epoch latent vectors\n",
        "                adjacency_matrix_1 = get_adjacency_matrix(model1.history['latent'][-1])\n",
        "                adjacency_matrix_2 = get_adjacency_matrix(model2.history['latent'][-1])\n",
        "                spectral_dists.append(adjacency_spectral_distance(adjacency_matrix_1, adjacency_matrix_2))\n",
        "                # Get absolute loss difference\n",
        "                loss_diffs.append(abs(model1.history['train']['loss'][-1] - model2.history['train']['loss'][-1]))\n",
        "\n",
        "    # Plot spectral distance vs. loss difference\n",
        "    plt.scatter(spectral_dists, loss_diffs, s=2)\n",
        "    plt.xlabel('Spectral distance')\n",
        "    plt.ylabel('Loss difference')\n",
        "    plt.title(f\"Spectral distance vs. loss difference after {len(model1.history['train']['loss']) - 1} epochs\")\n",
        "\n",
        "    # Save plot to file\n",
        "    plt.savefig(path);\n",
        "    plt.show();\n",
        "else:\n",
        "    # Load plot from file\n",
        "    print('Loading image from file.')\n",
        "    plt.axes([0,0,1,1])\n",
        "    plt.imshow(mpimg.imread(path))\n",
        "    plt.axis('off')\n",
        "    plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqiXN1UhomQt"
      },
      "outputs": [],
      "source": [
        "path = 'results/p2/initweights_dist_v_loss_diff.png'\n",
        "if not os.path.exists(path):\n",
        "    # Plot Frobenius distance of initial weights versus loss difference for each pair of models\n",
        "    frobenius_dists = []\n",
        "    loss_diffs = []\n",
        "    for i, model1 in enumerate(models_list):\n",
        "        for j, model2 in enumerate(models_list):\n",
        "            if i < j:\n",
        "                # Get first hidden layer initial weights for each model\n",
        "                init_weights_i = model1.history[\"weights\"][0][0]\n",
        "                init_weights_j = model2.history[\"weights\"][0][0]\n",
        "                # Get Frobenius distance\n",
        "                frobenius_dists.append(np.linalg.norm(init_weights_i - init_weights_j))\n",
        "                # Get absolute loss difference\n",
        "                loss_diffs.append(abs(model1.history['train']['loss'][-1] - model2.history['train']['loss'][-1]))\n",
        "\n",
        "    # Plot Frobenius distance vs. loss difference\n",
        "    plt.scatter(frobenius_dists, loss_diffs, s=2)\n",
        "    plt.xlabel('Frobenius distance')\n",
        "    plt.ylabel('Loss difference')\n",
        "    plt.title(f\"Frobenius distance vs. loss difference after {len(model1.history['train']['loss']) - 1} epochs\")\n",
        "\n",
        "    # Save plot to file\n",
        "    plt.savefig(path);\n",
        "    plt.show();\n",
        "else:\n",
        "    # Load plot from file\n",
        "    print('Loading image from file.')\n",
        "    plt.axes([0,0,1,1])\n",
        "    plt.imshow(mpimg.imread(path))\n",
        "    plt.axis('off')\n",
        "    plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtGEOCCNomQu"
      },
      "outputs": [],
      "source": [
        "path = 'results/p2/heatmaps.png'\n",
        "if not os.path.exists(path):\n",
        "    # Plot heatmap of adjacency matrix for each model in models_list\n",
        "    nrows = 10\n",
        "    ncols = 10\n",
        "    fig, ax = plt.subplots(nrows, ncols, tight_layout=True, figsize = (ncols * 5, nrows * 5))\n",
        "    ax = ax.flatten()\n",
        "\n",
        "    for i, model in enumerate(models_list):\n",
        "        # Get adjacency matrix\n",
        "        adjacency_matrix = get_adjacency_matrix(model.history['latent'][-1])\n",
        "        # Plot heatmap\n",
        "        ax[i].imshow(adjacency_matrix, cmap='magma')\n",
        "        ax[i].set_title(f'Last epoch training accuracy: {model.history[\"train\"][\"acc\"][-1]:.2f}')\n",
        "        \n",
        "    # Save plot to file\n",
        "    plt.savefig(path);\n",
        "    plt.show();\n",
        "else:\n",
        "    # Load plot from file\n",
        "    print('Loading image from file.')\n",
        "    plt.axes([0,0,1,1])\n",
        "    plt.imshow(mpimg.imread(path))\n",
        "    plt.axis('off')\n",
        "    plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUmwC0d4omQu"
      },
      "outputs": [],
      "source": [
        "path = 'results/p2/pca.png'\n",
        "\n",
        "# KMeans clustering of LGGs via adjacency matrices\n",
        "# Plot PCA of adjacency matrices\n",
        "if not os.path.exists(path):\n",
        "    # Get adjacency matrices\n",
        "    adjacency_matrices = []\n",
        "    for model in models_list:\n",
        "        adjacency_matrices.append(get_adjacency_matrix(model.history['latent'][-1]))\n",
        "\n",
        "    # Convert to dataframe\n",
        "    adj_df = []\n",
        "    for mat in adjacency_matrices:\n",
        "        adj_dict = {}\n",
        "        for i in range(10):\n",
        "            for j in range(i+1, 10):\n",
        "                adj_dict[f'{i}_{j}'] = mat[i][j]\n",
        "        adj_df.append(adj_dict)\n",
        "    adj_df = pd.DataFrame(adj_df)\n",
        "\n",
        "    # KMeans clustering\n",
        "    fit = KMeans(n_clusters = 8, init = 'random', n_init=10, random_state=1).fit(adj_df)\n",
        "\n",
        "    # Compute PCA\n",
        "    fitted_pca = PCA().fit(adj_df)\n",
        "    pca_df = pd.DataFrame(fitted_pca.transform(adj_df), columns=[f'PCA{i}' for i in range(len(adj_df.columns.values))])\n",
        "    pca_df['label'] = fit.labels_\n",
        "\n",
        "    # Plot PCA\n",
        "    for cluster_id, cluster_df in pca_df.groupby(['label']):\n",
        "        pca1 = cluster_df.iloc[:,0]\n",
        "        pca2 = cluster_df.iloc[:,1]\n",
        "        plt.scatter(pca1, pca2, label=cluster_id)\n",
        "\n",
        "    plt.xlabel(f'PC1 ({np.round(fitted_pca.explained_variance_ratio_[0], 1)})')\n",
        "    plt.ylabel(f'PC2 ({np.round(fitted_pca.explained_variance_ratio_[1], 1)})')\n",
        "    plt.legend()\n",
        "\n",
        "    # Save plot to file\n",
        "    plt.savefig(path);\n",
        "    plt.show();\n",
        "else:\n",
        "    # Load plot from file\n",
        "    print('Loading image from file.')\n",
        "    plt.axes([0,0,1,1])\n",
        "    plt.imshow(mpimg.imread(path))\n",
        "    plt.axis('off')\n",
        "    plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot latent spaces for all models in each cluster\n",
        "\n",
        "for cluster_id, cluster_df in pca_df.groupby(['label']):\n",
        "\n",
        "    path = f'results/p2/latent_cLGGclust{cluster_id}'\n",
        "    if not os.path.exists(path):\n",
        "\n",
        "        cluster_models_idx = pca_df.loc[pca_df['label'] == cluster_id,:].index\n",
        "        cluster_models = [models_list[idx] for idx in cluster_models_idx]\n",
        "\n",
        "        # Visualize final latent space for each model\n",
        "        ncols = 10\n",
        "        nrows = math.ceil(len(cluster_models)/ncols)\n",
        "        fig, ax = plt.subplots(nrows, ncols, tight_layout=True, figsize = (ncols * 5, nrows * 5))\n",
        "        ax = ax.flatten()\n",
        "\n",
        "        colors = sns.color_palette(['#e6194B','#f58231','#ffe119','#bfef45','#3cb44b','#42d4f4','#4363d8', '#000075', '#911eb4','#f032e6'])\n",
        "\n",
        "        for i, model in enumerate(cluster_models):\n",
        "            # Get latent vecs for final epoch\n",
        "            latent_vecs = model.history['latent'][-1]\n",
        "            # Create dataframe\n",
        "            df = []\n",
        "            for index, element in enumerate(latent_vecs[:10000]): # Limit to first 10000 example\n",
        "                vec, label = element\n",
        "                df.append({'x': vec[0], 'y': vec[1], 'digit': label})\n",
        "            df = pd.DataFrame(df)\n",
        "\n",
        "            # Plot latent vectors in 2D, color-coded by digit\n",
        "            # print(f'Plotting {len(df)} examples.')\n",
        "            sns.scatterplot(data=df, x=\"x\", y=\"y\", hue=\"digit\", palette=colors, ax=ax[i], legend=False)\n",
        "\n",
        "            # Add title\n",
        "            title = f'Model {cluster_models_idx[i]}; val_acc: {model.history[\"val\"][\"acc\"][-1]:.2f}'\n",
        "            ax[i].set_title(title)\n",
        "\n",
        "        # Create legend in last subplot\n",
        "        for i in range(10):\n",
        "            ax[-1].scatter([], [], color=colors[i], label=i)\n",
        "        ax[-1].legend()\n",
        "\n",
        "        # Save plot to file\n",
        "        plt.savefig(path);\n",
        "        plt.show();\n",
        "    else:\n",
        "        # Load plot from file\n",
        "        print('Loading image from file.')\n",
        "        plt.axes([0,0,1,1])\n",
        "        plt.imshow(mpimg.imread(path))\n",
        "        plt.axis('off')\n",
        "        plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot heatmap of adjacency matrix for all models in each cluster\n",
        "\n",
        "for cluster_id, cluster_df in pca_df.groupby(['label']):\n",
        "\n",
        "    path = f'results/p2/latent_cLGGclust{cluster_id}'\n",
        "    if not os.path.exists(path): \n",
        "\n",
        "        cluster_models_idx = pca_df.loc[pca_df['label'] == cluster_id,:].index\n",
        "        cluster_models = [models_list[idx] for idx in cluster_models_idx]\n",
        "\n",
        "        ncols = 10\n",
        "        nrows = math.ceil(len(cluster_models)/ncols)\n",
        "        fig, ax = plt.subplots(nrows, ncols, tight_layout=True, figsize = (ncols * 5, nrows * 5))\n",
        "        ax = ax.flatten()\n",
        "\n",
        "        for i, model in enumerate(cluster_models):\n",
        "            # Get adjacency matrix\n",
        "            adjacency_matrix = get_adjacency_matrix(model.history['latent'][-1])\n",
        "            # Plot heatmap\n",
        "            ax[i].imshow(adjacency_matrix, cmap='magma')\n",
        "            ax[i].set_title(f'Last epoch training accuracy: {model.history[\"train\"][\"acc\"][-1]:.2f}')\n",
        "            \n",
        "        # Save plot to file\n",
        "        plt.savefig(path);\n",
        "        plt.show();\n",
        "    else:\n",
        "        # Load plot from file\n",
        "        print('Loading image from file.')\n",
        "        plt.axes([0,0,1,1])\n",
        "        plt.imshow(mpimg.imread(path))\n",
        "        plt.axis('off')\n",
        "        plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aT_XY40tpuoP"
      },
      "outputs": [],
      "source": [
        "path = 'results/p2/cluster_dist_hist.png'\n",
        "\n",
        "# For every pair of clusters, plot histogram of distance between cluster centroids\n",
        "if not os.path.exists(path):\n",
        "    nrows = 5\n",
        "    ncols = 9\n",
        "    fig, ax = plt.subplots(nrows, ncols, tight_layout=True, figsize = (ncols * 5, nrows * 5))\n",
        "    ax = ax.flatten()\n",
        "\n",
        "    # Get adjacency matrices for each model\n",
        "    adjacency_matrices = []\n",
        "    for model in models_list:\n",
        "        adjacency_matrices.append(get_adjacency_matrix(model.history['latent'][-1]))\n",
        "\n",
        "    i = 0\n",
        "    for digit1 in range(10):\n",
        "        for digit2 in range(digit1 + 1, 10):\n",
        "            distances = []\n",
        "            for j, model in enumerate(models_list):\n",
        "                # Get separation between digit1 and digit2 cluster centroids from adjacency matrix\n",
        "                distances.append(adjacency_matrices[j][digit1][digit2])\n",
        "\n",
        "            # Plot histogram\n",
        "            ax[i].hist(distances, bins=20)\n",
        "            ax[i].set_title(f'{digit1} vs. {digit2}')\n",
        "            ax[i].set_xlabel('Distance between cluster centroids')\n",
        "            i += 1\n",
        "    \n",
        "    # Add title\n",
        "    title = 'Cluster separation across randomly initialized models'\n",
        "    fig.suptitle(title)\n",
        "\n",
        "    # Save plot to file\n",
        "    plt.savefig(path)\n",
        "    plt.show();\n",
        "else:\n",
        "    # Load plot from file\n",
        "    print('Loading image from file.')\n",
        "    plt.axes([0,0,1,1])\n",
        "    plt.imshow(mpimg.imread(path))\n",
        "    plt.axis('off')\n",
        "    plt.show();            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffkzzfympyTL"
      },
      "outputs": [],
      "source": [
        "path = 'results/p2/inertia.png'\n",
        "# Plot inertia of each cluster over epochs\n",
        "if not os.path.exists(path):\n",
        "    nrows = 10\n",
        "    ncols = 10\n",
        "    fig, ax = plt.subplots(nrows, ncols, tight_layout=True, figsize = (ncols * 5, nrows * 5))\n",
        "    ax = ax.flatten()\n",
        "\n",
        "    colors = sns.color_palette(['#e6194B','#f58231','#ffe119','#bfef45','#3cb44b','#42d4f4','#4363d8', '#000075', '#911eb4','#f032e6'])\n",
        "\n",
        "    for n, model in enumerate(models_list):\n",
        "        # Get list of cluster inertias for each epoch\n",
        "        cluster_inertias = []\n",
        "        for epoch in range(len(model.history['latent'])):\n",
        "            cluster_inertias.append(get_cluster_inertias(model.history['latent'][epoch]))\n",
        "        \n",
        "        # Convert to numpy array and transpose\n",
        "        cluster_inertias = np.array(cluster_inertias).T\n",
        "\n",
        "        # Plot each row of cluster inertias in different colors\n",
        "        for i in range(10):\n",
        "            ax[n].plot(cluster_inertias[i], color=colors[i], label=i)\n",
        "        ax[n].set_xlabel('Epoch')\n",
        "        ax[n].set_ylabel('Cluster inertia')\n",
        "        ax[n].set_title(f'val_acc: {model.history[\"val\"][\"acc\"][-1]:.2f}')\n",
        "        ax[n].legend()\n",
        "\n",
        "    # Save plot to file\n",
        "    plt.savefig(path);\n",
        "    plt.show();\n",
        "else:\n",
        "    # Load plot from file\n",
        "    print('Loading image from file.')\n",
        "    plt.axes([0,0,1,1])\n",
        "    plt.imshow(mpimg.imread(path))\n",
        "    plt.axis('off')\n",
        "    plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChTf-DuOomQl"
      },
      "source": [
        "## Part 3. Linear traversal of weights hyperspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YX2nMbIyomQl"
      },
      "outputs": [],
      "source": [
        "num_models = 100\n",
        "path = 'results/p3'\n",
        "\n",
        "# Set seed for reproducibility\n",
        "np.random.seed(1)\n",
        "\n",
        "# Initialize weights array of shape (num_neurons_i, num_neurons_i-1) for each layer i except the input layer\n",
        "weights = [np.random.randn(y, x) for y, x in zip(sizes[1:], sizes[:-1])]\n",
        "\n",
        "# Initialize biases array of shape (num_neurons_i, 1) for each layer i except the input layer\n",
        "biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "\n",
        "# Initialize list of data indices for data shuffling per epoch\n",
        "shuffle_order = [np.random.permutation(int(len(train_data)*(1 - validation_size))) for i in range(epochs)]\n",
        "\n",
        "# Train models with different initial weights of first neuron for first pixel in first hidden layer\n",
        "models_list = []\n",
        "for idx, variable_weight in enumerate(np.linspace(-10, 10, num_models)):\n",
        "    if not os.path.exists(f'{path}/model_{idx}.pkl.gz'):\n",
        "        # Edit initial weight matrix\n",
        "        new_weights = weights.copy()\n",
        "        new_weights[2][0][0] = variable_weight\n",
        "        \n",
        "        # Build & train model\n",
        "        print(f'Building model {idx} with initial weight[2][0][0]: {new_weights[2][0][0]:.4f}...')\n",
        "        model = Model(sizes, weights=new_weights, biases=biases)\n",
        "        model.fit(train_data, epochs = epochs, batch_size = batch_size, learning_rate = learning_rate, decay_strength = decay_strength, validation_size = validation_size, shuffle_order = shuffle_order, store_latent_vecs = True, verbose=0)\n",
        "        models_list.append(deepcopy(model)) # Must deepcopy model to avoid overwriting previous models\n",
        "\n",
        "        # Save model to file\n",
        "        print(f'Saving model {idx} to file...')\n",
        "        start_time = time.time()\n",
        "        with gzip.open(f'{path}/model_{idx}.pkl.gz', 'wb') as f:\n",
        "            pickle.dump(model, f)\n",
        "        print(f'Done. Saving took {time.time() - start_time:.2f} seconds.')\n",
        "    else:\n",
        "    # Open model from file\n",
        "        print(f'Opening model {idx} from file...')\n",
        "        start_time = time.time()\n",
        "        with gzip.open(f'{path}/model_{idx}.pkl.gz', 'rb') as f:\n",
        "            models_list.append(pickle.load(f))\n",
        "        print(f'Done. Opening took {time.time() - start_time:.2f} seconds.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehMlw0KVomQm"
      },
      "outputs": [],
      "source": [
        "path = 'results/p3/acc.png'\n",
        "if not os.path.exists(path):\n",
        "    # Plot histogram of final training accuracy\n",
        "    plt.hist([model.history['train']['acc'][-1] for model in models_list], bins=10)\n",
        "    plt.xlabel('Final training accuracy')\n",
        "    plt.ylabel('Number of models')\n",
        "    plt.title(f'Variation in training accuracy after {len(models_list[0].history[\"train\"][\"loss\"]) - 1} epochs')\n",
        "    plt.savefig(path);\n",
        "    plt.show();\n",
        "else:\n",
        "    # Load plot from file\n",
        "    print('Loading image from file.')\n",
        "    plt.axes([0,0,1,1])\n",
        "    plt.imshow(mpimg.imread(path))\n",
        "    plt.axis('off')\n",
        "    plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bjiE5pPomQn"
      },
      "outputs": [],
      "source": [
        "path = 'results/p3/history.png'\n",
        "if not os.path.exists(path):\n",
        "    # Plot training history for each model\n",
        "    nrows = 10\n",
        "    ncols = 10\n",
        "    fig, ax = plt.subplots(nrows, ncols, tight_layout=True, figsize = (ncols * 5, nrows * 5))\n",
        "    ax = ax.flatten()\n",
        "\n",
        "    for i, model in enumerate(models_list):\n",
        "        # Plot accuracy history\n",
        "        ax[i].plot(range(len(model.history['val']['acc'])), model.history['val']['acc'], label='Validation')\n",
        "        ax[i].plot(range(len(model.history['train']['acc'])), model.history['train']['acc'], label='Train')\n",
        "        ax[i].set_xticks(np.arange(0, len(model.history['val']['acc']), 10)) # Set xticks\n",
        "        ax[i].set_ylabel('Accuracy') # Y axis label\n",
        "        ax[i].set_xlabel('Epochs') # X axis label\n",
        "        ax[i].legend() # Add legend\n",
        "\n",
        "        # Add title\n",
        "        title = f'Weight[2][0][0] = {model.history[\"weights\"][0][2][0][0]:.3f}'\n",
        "        ax[i].set_title(title)\n",
        "\n",
        "    # Save plot to file\n",
        "    plt.savefig(path);\n",
        "    plt.show();\n",
        "else:\n",
        "    # Load plot from file\n",
        "    print('Loading image from file.')\n",
        "    plt.axes([0,0,1,1])\n",
        "    plt.imshow(mpimg.imread(path))\n",
        "    plt.axis('off')\n",
        "    plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbdUxjybomQn"
      },
      "outputs": [],
      "source": [
        "path = 'results/p3/latent.png'\n",
        "if not os.path.exists(path):\n",
        "    # Visualize final latent space for each model\n",
        "    nrows = 10\n",
        "    ncols = 10\n",
        "    fig, ax = plt.subplots(nrows, ncols, tight_layout=True, figsize = (ncols * 5, nrows * 5))\n",
        "    ax = ax.flatten()\n",
        "\n",
        "    colors = sns.color_palette(['#e6194B','#f58231','#ffe119','#bfef45','#3cb44b','#42d4f4','#4363d8', '#000075', '#911eb4','#f032e6'])\n",
        "\n",
        "    for i, model in enumerate(models_list):\n",
        "        # Get latent vecs for final epoch\n",
        "        latent_vecs = model.history['latent'][-1]\n",
        "        # Create dataframe\n",
        "        df = []\n",
        "        for index, element in enumerate(latent_vecs[:10000]): # Limit to first 10000 example\n",
        "            vec, label = element\n",
        "            df.append({'x': vec[0], 'y': vec[1], 'digit': label})\n",
        "        df = pd.DataFrame(df)\n",
        "\n",
        "        # Plot latent vectors in 2D, color-coded by digit\n",
        "        # print(f'Plotting {len(df)} examples.')\n",
        "        sns.scatterplot(data=df, x=\"x\", y=\"y\", hue=\"digit\", palette=colors, ax=ax[i], legend=False)\n",
        "\n",
        "        # Add title\n",
        "        title = f'weight[2][0][0] = {model.history[\"weights\"][0][2][0][0]:.3f}; val_acc: {model.history[\"val\"][\"acc\"][-1]:.2f}'\n",
        "        ax[i].set_title(title)\n",
        "\n",
        "    # Create legend in last subplot\n",
        "    for i in range(10):\n",
        "        ax[-1].scatter([], [], color=colors[i], label=i)\n",
        "    ax[-1].legend()\n",
        "\n",
        "    # Save plot to file\n",
        "    plt.savefig(path);\n",
        "    plt.show();\n",
        "else:\n",
        "    # Load plot from file\n",
        "    print('Loading image from file.')\n",
        "    plt.axes([0,0,1,1])\n",
        "    plt.imshow(mpimg.imread(path))\n",
        "    plt.axis('off')\n",
        "    plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHmWcsHZomQo"
      },
      "outputs": [],
      "source": [
        "path = 'results/p3/cLGG_distance_v_weights_delta.png'\n",
        "if not os.path.exists(path):\n",
        "    # Plot latent space similarity (by adjacency spectral distance) versus difference in initial weights, for each pair of models in models_list\n",
        "    spectral_distances = []\n",
        "    weight_deltas = []\n",
        "    for i in range(len(models_list)):\n",
        "        for j in range(i + 1, len(models_list)):\n",
        "            # Get adjacency spectral distances of last epoch latent vectors\n",
        "            adjacency_matrix_1 = get_adjacency_matrix(models_list[i].history['latent'][-1])\n",
        "            adjacency_matrix_2 = get_adjacency_matrix(models_list[j].history['latent'][-1])\n",
        "            spectral_distances.append(adjacency_spectral_distance(adjacency_matrix_1, adjacency_matrix_2))\n",
        "\n",
        "            # Get absolute value difference between initial weights of first neuron for first pixel in first hidden layer\n",
        "            weight_difference = np.abs(models_list[i].history[\"weights\"][0][2][0][0] - models_list[j].history[\"weights\"][0][2][0][0])\n",
        "            weight_deltas.append(weight_difference)\n",
        "\n",
        "    # Plot spectral distance vs. change in training loss\n",
        "    # Color by epoch\n",
        "    plt.scatter(spectral_distances, weight_deltas, s=1)\n",
        "    plt.xlabel('Spectral distance')\n",
        "    plt.ylabel('Difference in initial weight of first neuron in bottleneck layer')\n",
        "    plt.title('Spectral distance vs. difference in single initial weight')\n",
        "\n",
        "    # Save plot to file\n",
        "    plt.savefig(path, bbox_inches='tight')\n",
        "    plt.show();\n",
        "else:\n",
        "    # Load plot from file\n",
        "    print('Loading image from file.')\n",
        "    plt.axes([0,0,1,1])\n",
        "    plt.imshow(mpimg.imread(path))\n",
        "    plt.axis('off')\n",
        "    plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rieSkDDgomQp"
      },
      "outputs": [],
      "source": [
        "path = 'results/p3/adjacency_mean_v_loss.png'\n",
        "if not os.path.exists(path):\n",
        "    # Plot mean adjacency versus final training loss for each model in models_list\n",
        "    \n",
        "    # Calculate mean adjacency for each model\n",
        "    adjacency_means = []\n",
        "    for model in models_list:\n",
        "        adjacency_means.append(np.mean(get_adjacency_matrix(model.history['latent'][-1])))\n",
        "    \n",
        "    # Get final training loss for each model\n",
        "    final_losses = []\n",
        "    for model in models_list:\n",
        "        final_losses.append(model.history['train']['loss'][-1])\n",
        "\n",
        "    # Plot mean adjacency versus final training loss\n",
        "    plt.scatter(adjacency_means, final_losses)\n",
        "    plt.xlabel('Adjacency mean')\n",
        "    plt.ylabel('Final training loss')\n",
        "    plt.title('Mean adjacency versus final training loss')\n",
        "\n",
        "    # Save plot to file\n",
        "    plt.savefig(path, bbox_inches='tight')\n",
        "    plt.show();\n",
        "else:\n",
        "    # Load plot from file\n",
        "    print('Loading image from file.')\n",
        "    plt.axes([0,0,1,1])\n",
        "    plt.imshow(mpimg.imread(path))\n",
        "    plt.axis('off')\n",
        "    plt.show();"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "0d02bebebd0d339a7f7e0cbd8d22cc3c8fca77e19de36675d07210a32ba28402"
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 ('_stochastic_env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
