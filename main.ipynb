{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle, gzip\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from scipy.special import expit # Vectorized sigmoid function\n",
    "from scipy.special import softmax as softmax_\n",
    "\n",
    "def show(image):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    new = image.copy()\n",
    "    if image.shape == 3:                    # Switch R and B channels so it shows up as correctly as R,G,B, if image is 3-channel\n",
    "        new[:,:,0] = image[:,:,2]\n",
    "        new[:,:,2] = image[:,:,0]\n",
    "    plt.imshow(new, cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "\n",
    "with gzip.open('data/mnist.pkl.gz', 'rb') as f:\n",
    "    train_data, val_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
    "    # train_data: tuple of (x_train, y_train), where x_train.shape = (50000, 784) and y_train.shape = (50000, 1)\n",
    "    # val_data: tuple of (x_val, y_val), where x_val.shape = (10000, 784) and y_val.shape = (10000, 1)\n",
    "    # test_data: tuple of (x_test, y_test), where x_test.shape = (10000, 784) and y_test.shape = (10000, 1)\n",
    "\n",
    "x_train, y_train = train_data\n",
    "x_val, y_val = val_data\n",
    "x_test, y_test = test_data\n",
    "\n",
    "# Combine training and validation data\n",
    "x_train = np.concatenate((x_train, x_val), axis=0)\n",
    "y_train = np.concatenate((y_train, y_val), axis=0)\n",
    "\n",
    "# Reshape x\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train = np.eye(10)[y_train].reshape((-1, 10, 1))\n",
    "y_test = np.eye(10)[y_test].reshape((-1, 10, 1))\n",
    "\n",
    "# Zip data and labels into tuples\n",
    "train_data = list(zip(x_train, y_train))\n",
    "test_data = list(zip(x_test, y_test))\n",
    "\n",
    "# Shuffle training data\n",
    "np.random.shuffle(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructor for neural network\n",
    "# Adapted from Nielsen textbook http://neuralnetworksanddeeplearning.com/chap1.html and https://towardsdatascience.com/mnist-handwritten-digits-classification-from-scratch-using-python-numpy-b08e401c4dab and https://github.com/geohot/ai-notebooks/blob/master/mnist_from_scratch.ipynb\n",
    "\n",
    "class Model(object):\n",
    "\n",
    "    # Fully connected neural network with layer i having sizes[i] neurons\n",
    "    def __init__(self, sizes, weights=None, biases=None):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "\n",
    "        # Initialize weights & biases if not provided\n",
    "        if weights is None:\n",
    "            self.weights = [np.random.randn(y, x) for y, x in zip(sizes[1:], sizes[:-1])] # Initialize weights array of shape (num_neurons_i, num_neurons_i-1) for each layer i except the input layer\n",
    "        else:\n",
    "            self.weights = weights\n",
    "        \n",
    "        if biases is None:\n",
    "            self.biases = [np.random.randn(y, 1) for y in sizes[1:]] # Initialize biases array of shape (num_neurons_i, 1) for each layer i except the input layer\n",
    "        else:\n",
    "            self.biases = biases\n",
    "\n",
    "        # Store initial weights and biases\n",
    "        self.initial_weights = self.weights\n",
    "        self.initial_biases = self.biases\n",
    "        \n",
    "        # Initialize training history\n",
    "        self.history = {'train': {'acc':[], 'loss':[]}, 'val': {'acc':[], 'loss':[]}, 'latent':[], 'weights':[], 'biases':[]}\n",
    "\n",
    "        # Number of parameters\n",
    "        print(f'Number of model parameters: {sum(np.prod(w.shape) for w in self.weights) + sum(np.prod(b.shape) for b in self.biases)}')\n",
    "\n",
    "    # Activation functions\n",
    "    # x is a matrix with nrows = num_hidden_neurons and ncols = batch_size generated by hidden layer affine transformation\n",
    "    def activation(self, x):\n",
    "        # def sigmoid(x):\n",
    "        #     # Overflow-safe elementwise sigmoid function (https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/)\n",
    "        #     if x > 0:\n",
    "        #         return 1 / (1 + np.exp(-x))\n",
    "        #     else:\n",
    "        #         return np.exp(x) / (1 + np.exp(x))\n",
    "        # return np.vectorize(sigmoid)(x) # Vectorize sigmoid function to apply to each element of matrix (10x slower than applying non-overflow-safe fx to whole array)\n",
    "        return np.exp(x) / (1 + np.exp(x))\n",
    "\n",
    "    # Derivative of activation function\n",
    "    def activation_der(self, x):\n",
    "        return self.activation(x) * (1 - self.activation(x))\n",
    "\n",
    "    # Softmax function\n",
    "    # Overflow-safe softmax function (https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/)\n",
    "    def softmax(self, x):\n",
    "        # x is a matrix with nrows = num_hidden_neurons and ncols = batch_size generated by hidden layer affine transformation\n",
    "        # return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return softmax_(x, axis = 0)\n",
    "        # return np.exp(x) / (1 + np.exp(x))\n",
    "\n",
    "    # Derivative of softmax function\n",
    "    def softmax_der(self, x):\n",
    "        return self.softmax(x) * (1 - self.softmax(x))        \n",
    "\n",
    "    # Loss function\n",
    "    def loss(self, y_true, y_pred):\n",
    "        return np.mean(np.sum(np.nan_to_num(-y_true*np.log(y_pred)-(1-y_true)*np.log(1-y_pred)), axis = 0)) # Cross entropy loss\n",
    "        # return np.mean(((y_true - y_pred)**2)) # mean squared error\n",
    "\n",
    "    # Derivative of loss function\n",
    "    def loss_der(self, y_true, y_pred):\n",
    "        return y_pred - y_true # Derivative of cross entropy loss\n",
    "        # return 2*(y_pred - y_true) # Derivative of mean squared error\n",
    "\n",
    "    # Backpropagation\n",
    "    # Input x,y are batch matrices with each column being the vector of a single training example\n",
    "    # Returns (nabla_b, nabla_w), where nabla_b is list of gradients of cost with respect to biases (one matrix for each layer) and nabla_w is list of gradients of cost with respect to weights (one matrix for each layer)\n",
    "    def backprop(self, x, y):\n",
    "        # Initialize lists of gradients for each layer\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        ###### Forward pass, storing weighted inputs (z) and activations for each layer ######\n",
    "        activation = x # Initialize activation with input\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the weighted input z vectors, layer by layer\n",
    "        # Iterate through each layer\n",
    "        for i, tup in enumerate(zip(self.biases, self.weights)):\n",
    "            b, w = tup # Unpack biases and weights\n",
    "            z = np.dot(w, activation) + b # Compute weighted input z\n",
    "            zs.append(z) # Store weighted input z          \n",
    "            if i == (self.num_layers - 2):\n",
    "                # If last layer, apply softmax\n",
    "                activation = self.softmax(z)\n",
    "            else:\n",
    "                # If not last layer, apply activation function\n",
    "                activation = self.activation(z)\n",
    "            activations.append(activation) # Store activation\n",
    "\n",
    "        ###### Backward pass, computing gradients of cost with respect to biases and weights ######\n",
    "        # Get gradients for output layer\n",
    "        delta = self.loss_der(y, activations[-1]) * self.softmax_der(zs[-1]) # Hadamard product of loss gradient and softmax derivative for output layer\n",
    "        nabla_b[-1] = delta # Store gradient of cost with respect to biases of last layer\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose()) # Store gradient of cost with respect to weights of last layer\n",
    "        # Iterate through each layer in reverse order, starting from second to last layer\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l] # Retrieve weighted input z for current layer\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * self.activation_der(z) # Compute gradient of cost with respect to weighted input z\n",
    "            nabla_b[-l] = delta # Store gradient of cost with respect to biases of current layer\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose()) # Store gradient of cost with respect to weights of current layer\n",
    "\n",
    "        # Sum columns of nabla_b matrices to get overall bias gradients for batch\n",
    "        nabla_b = [np.sum(nabla_b[i], axis=1).reshape(nabla_b[i].shape[0], 1) for i in range(len(nabla_b))]\n",
    "\n",
    "        # Get training performance for batch\n",
    "        acc = np.mean(np.argmax(activations[-1], axis=0) == np.argmax(y, axis=0)) # For each column of activations[-1], get predicted label and compare to true label\n",
    "        loss = self.loss(y, activations[-1]) # Compute loss\n",
    "        \n",
    "        return nabla_b, nabla_w, acc, loss\n",
    "\n",
    "    # Model training using SGD\n",
    "    # training_data is a list of tuples (x, y) representing the training inputs and the desired outputs\n",
    "    def fit(self, training_data, epochs = 10, batch_size = 10, learning_rate = 1, decay_strength = 1, validation_size=0.3, shuffle_order=None, store_latent_vecs=False, verbose=2):\n",
    "        \n",
    "        # Take last portion of training data as validation data\n",
    "        training_data = training_data[:-int(validation_size*len(training_data))]\n",
    "        val_data = training_data[-int(validation_size*len(training_data)):]\n",
    "\n",
    "        x_train = np.array([x for x, y in training_data])[:,:,0].T # Retrieve matrix of training examples where each column is a training example\n",
    "        y_train = np.array([y for x, y in training_data])[:,:,0].T # Retrieve matrix of training labels where each column is the one-hot encoded vector\n",
    "\n",
    "        ## Store baseline model characteristics (before training)\n",
    "        # Evaluate model on training data\n",
    "        train_acc, train_loss = self.evaluate(training_data)\n",
    "        # Evaluate model on validation data\n",
    "        val_acc, val_loss = self.evaluate(val_data)\n",
    "        print(f\"Baseline characteristics: Training accuracy: {train_acc:.4f}. Validation accuracy: {val_acc:.4f}. Training loss: {train_loss:.4f}. Validation loss: {val_loss:.4f}\")\n",
    "        # Update global history\n",
    "        self.history['train']['acc'].append(train_acc)\n",
    "        self.history['train']['loss'].append(train_loss)\n",
    "        self.history['val']['acc'].append(val_acc)\n",
    "        self.history['val']['loss'].append(val_loss)\n",
    "        # Store latent vectors\n",
    "        if store_latent_vecs:\n",
    "            vecs = self.feedforward(x_train, layer_num = 2).T\n",
    "            labs = np.argmax(y_train, axis=0)\n",
    "            self.history['latent'].append(list(zip(vecs, labs))) # Store latent representation of training data before training\n",
    "        # Store layer weights\n",
    "        self.history['weights'].append(self.weights)\n",
    "        # Store layer biases\n",
    "        self.history['biases'].append(self.biases)\n",
    "\n",
    "        # Iterate through each epoch\n",
    "        print(f'Training on {len(training_data)} examples, validating on {len(val_data)} examples')\n",
    "        for j in range(epochs):\n",
    "            # Initialize epoch training accuracy and loss history\n",
    "            train_acc = []\n",
    "            train_loss = []\n",
    "            # Get time\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Adjust learning rate for current epoch based on decay strength\n",
    "            learning_rate_adj = learning_rate/(1 + decay_strength * j)\n",
    "\n",
    "            # Shuffle training data in preparation for SGD batching\n",
    "            if shuffle_order is None:\n",
    "                np.random.shuffle(training_data) # Shuffle training data randomly\n",
    "            else:\n",
    "                training_data = np.array(training_data, dtype=object)[shuffle_order[j]] # Shuffle training data according to order provided\n",
    "            # Construct training batches for SGD\n",
    "            batches = [training_data[k:k+batch_size] for k in range(0, len(training_data), batch_size)]\n",
    "            # Iterate through all batches\n",
    "            for batch in batches:\n",
    "                x_batch = np.array([x for x, y in batch])[:,:,0].T # Retrieve matrix of training examples where each column is a training example\n",
    "                y_batch = np.array([y for x, y in batch])[:,:,0].T # Retrieve matrix of training labels where each column is the one-hot encoded vector\n",
    "\n",
    "                nabla_b_, nabla_w_, acc, loss = self.backprop(x_batch, y_batch) # Compute loss gradients for each layer for whole batch\n",
    "\n",
    "                # Update epoch training accuracy and loss history for whole batch\n",
    "                train_acc.append(acc)\n",
    "                train_loss.append(loss)\n",
    "\n",
    "                # Update weights and biases for each layer using average loss gradients\n",
    "                self.weights = [w-(learning_rate_adj*nw/batch_size) for w, nw in zip(self.weights, nabla_w_)] # Need to divide by batch size to get average gradient per example\n",
    "                self.biases = [b-(learning_rate_adj*nb/batch_size) for b, nb in zip(self.biases, nabla_b_)] # Need to divide by batch size to get average gradient per example\n",
    "            \n",
    "            # Compute epoch training accuracy and loss\n",
    "            train_acc, train_loss = np.mean(train_acc), np.mean(train_loss)\n",
    "            # Evaluate model on validation data\n",
    "            val_acc, val_loss = self.evaluate(val_data)\n",
    "            # Get time\n",
    "            end_time = time.time()\n",
    "            # Print epoch number (epochs start at 1)\n",
    "            if verbose >= 2:\n",
    "                print(f\"Epoch {j+1} complete. Time taken: {end_time - start_time:.2f} seconds.\")\n",
    "            if verbose >= 1:\n",
    "                print(f\"Learning rate: {learning_rate_adj:.4f}. Training accuracy: {train_acc:.4f}. Validation accuracy: {val_acc:.4f}. Mean training loss: {train_loss:.4f}. Mean validation loss: {val_loss:.4f}. Mean weight gradients: {[np.around(np.mean(np.abs(nw)), 4) for nw in nabla_w_]}.\")\n",
    "\n",
    "            # Update global history\n",
    "            self.history['train']['acc'].append(train_acc)\n",
    "            self.history['train']['loss'].append(train_loss)\n",
    "            self.history['val']['acc'].append(val_acc)\n",
    "            self.history['val']['loss'].append(val_loss)\n",
    "            # Store latent vectors\n",
    "            if store_latent_vecs:\n",
    "                vecs = self.feedforward(x_train, layer_num = 2).T\n",
    "                labs = np.argmax(y_train, axis=0)\n",
    "                self.history['latent'].append(list(zip(vecs, labs))) # Store latent representation of training data before training\n",
    "            # Store layer weights\n",
    "            self.history['weights'].append(self.weights)\n",
    "            # Store layer biases\n",
    "            self.history['biases'].append(self.biases)\n",
    "\n",
    "        print(\"Training complete.\")\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        x_test = np.array([x for x, y in test_data])[:,:,0].T # Retrieve matrix of training examples where each column is a training example\n",
    "        y_test = np.array([y for x, y in test_data])[:,:,0].T # Retrieve matrix of training labels where each column is the one-hot encoded vector\n",
    "\n",
    "        acc = np.mean(np.argmax(self.feedforward(x_test), axis=0) == np.argmax(y_test, axis=0))\n",
    "        loss = self.loss(y_test, self.feedforward(x_test)) \n",
    "\n",
    "        return acc, loss\n",
    "\n",
    "    # Get feedforward activations for given layer (default is last layer)\n",
    "    # Equivalent to Tensorflow get_layer()\n",
    "    def feedforward(self, x, layer_num = None):\n",
    "        # Dynamically update activations for each layer while moving forward through the network, starting with shape (input_size, 1) and ending with shape (output_size, 1)\n",
    "        if layer_num is None:\n",
    "            layer_num = self.num_layers - 1\n",
    "        activations = x\n",
    "        current_layer = 1\n",
    "        while current_layer <= layer_num:\n",
    "            # If current layer is last layer, return softmax activations\n",
    "            if current_layer == self.num_layers - 1:\n",
    "                activations = self.softmax(np.dot(self.weights[current_layer - 1], activations) + self.biases[current_layer - 1])\n",
    "            else:\n",
    "                activations = self.activation(np.dot(self.weights[current_layer - 1], activations) + self.biases[current_layer - 1])\n",
    "            current_layer += 1\n",
    "        return activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set global params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "sizes = [784, 30, 2, 10]\n",
    "epochs = 48\n",
    "batch_size = 60\n",
    "learning_rate = 5\n",
    "decay_strength = 1\n",
    "validation_size=0.3\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# Initialize weights array of shape (num_neurons_i, num_neurons_i-1) for each layer i except the input layer\n",
    "weights = [np.random.randn(y, x) for y, x in zip(sizes[1:], sizes[:-1])]\n",
    "\n",
    "# Initialize biases array of shape (num_neurons_i, 1) for each layer i except the input layer\n",
    "biases = [np.random.randn(y, 1) for y in sizes[1:]] \n",
    "\n",
    "# Initialize list of data indices for data shuffling per epoch\n",
    "shuffle_order = [np.random.permutation(int(len(train_data)*(1 - validation_size))) for i in range(epochs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that model training is reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model parameters: 23642\n",
      "Baseline characteristics: Training accuracy: 0.0991. Validation accuracy: 0.0969. Training loss: 4.5946. Validation loss: 4.5910\n",
      "Training on 42000 examples, validating on 12600 examples\n",
      "Epoch 1 complete. Time taken: 2.25 seconds.\n",
      "Learning rate: 5.0000. Training accuracy: 0.3160. Validation accuracy: 0.4110. Mean training loss: 2.7026. Mean validation loss: 2.2397. Mean weight gradients: [0.0039, 0.0815, 0.0642].\n",
      "Epoch 2 complete. Time taken: 2.01 seconds.\n",
      "Learning rate: 2.5000. Training accuracy: 0.4813. Validation accuracy: 0.5067. Mean training loss: 2.1382. Mean validation loss: 2.0499. Mean weight gradients: [0.0049, 0.181, 0.1341].\n",
      "Epoch 3 complete. Time taken: 1.66 seconds.\n",
      "Learning rate: 1.6667. Training accuracy: 0.5492. Validation accuracy: 0.5687. Mean training loss: 1.9916. Mean validation loss: 1.9299. Mean weight gradients: [0.0029, 0.2282, 0.1493].\n",
      "Epoch 4 complete. Time taken: 1.84 seconds.\n",
      "Learning rate: 1.2500. Training accuracy: 0.5813. Validation accuracy: 0.5817. Mean training loss: 1.8976. Mean validation loss: 1.8703. Mean weight gradients: [0.0036, 0.1547, 0.1381].\n",
      "Epoch 5 complete. Time taken: 2.08 seconds.\n",
      "Learning rate: 1.0000. Training accuracy: 0.5982. Validation accuracy: 0.6089. Mean training loss: 1.8311. Mean validation loss: 1.8008. Mean weight gradients: [0.0045, 0.2455, 0.1349].\n",
      "Epoch 6 complete. Time taken: 2.42 seconds.\n",
      "Learning rate: 0.8333. Training accuracy: 0.6127. Validation accuracy: 0.6119. Mean training loss: 1.7834. Mean validation loss: 1.7514. Mean weight gradients: [0.0036, 0.2358, 0.0583].\n",
      "Epoch 7 complete. Time taken: 2.24 seconds.\n",
      "Learning rate: 0.7143. Training accuracy: 0.6210. Validation accuracy: 0.6253. Mean training loss: 1.7468. Mean validation loss: 1.7260. Mean weight gradients: [0.0045, 0.1584, 0.1104].\n",
      "Epoch 8 complete. Time taken: 1.55 seconds.\n",
      "Learning rate: 0.6250. Training accuracy: 0.6300. Validation accuracy: 0.6346. Mean training loss: 1.7202. Mean validation loss: 1.7068. Mean weight gradients: [0.0047, 0.2665, 0.1028].\n",
      "Epoch 9 complete. Time taken: 1.85 seconds.\n",
      "Learning rate: 0.5556. Training accuracy: 0.6349. Validation accuracy: 0.6403. Mean training loss: 1.6981. Mean validation loss: 1.6833. Mean weight gradients: [0.0054, 0.1828, 0.0654].\n",
      "Epoch 10 complete. Time taken: 2.08 seconds.\n",
      "Learning rate: 0.5000. Training accuracy: 0.6426. Validation accuracy: 0.6460. Mean training loss: 1.6803. Mean validation loss: 1.6666. Mean weight gradients: [0.0067, 0.1876, 0.0485].\n",
      "Epoch 11 complete. Time taken: 1.37 seconds.\n",
      "Learning rate: 0.4545. Training accuracy: 0.6473. Validation accuracy: 0.6511. Mean training loss: 1.6650. Mean validation loss: 1.6529. Mean weight gradients: [0.0034, 0.3032, 0.076].\n",
      "Epoch 12 complete. Time taken: 1.28 seconds.\n",
      "Learning rate: 0.4167. Training accuracy: 0.6542. Validation accuracy: 0.6574. Mean training loss: 1.6502. Mean validation loss: 1.6385. Mean weight gradients: [0.0062, 0.4481, 0.0884].\n",
      "Epoch 13 complete. Time taken: 1.46 seconds.\n",
      "Learning rate: 0.3846. Training accuracy: 0.6622. Validation accuracy: 0.6533. Mean training loss: 1.6371. Mean validation loss: 1.6276. Mean weight gradients: [0.007, 0.3162, 0.0867].\n",
      "Epoch 14 complete. Time taken: 1.51 seconds.\n",
      "Learning rate: 0.3571. Training accuracy: 0.6655. Validation accuracy: 0.6690. Mean training loss: 1.6265. Mean validation loss: 1.6115. Mean weight gradients: [0.0045, 0.266, 0.0912].\n",
      "Epoch 15 complete. Time taken: 2.04 seconds.\n",
      "Learning rate: 0.3333. Training accuracy: 0.6747. Validation accuracy: 0.6734. Mean training loss: 1.6146. Mean validation loss: 1.6022. Mean weight gradients: [0.0052, 0.311, 0.1053].\n",
      "Epoch 16 complete. Time taken: 1.66 seconds.\n",
      "Learning rate: 0.3125. Training accuracy: 0.6831. Validation accuracy: 0.6826. Mean training loss: 1.6034. Mean validation loss: 1.5906. Mean weight gradients: [0.003, 0.2433, 0.0635].\n",
      "Epoch 17 complete. Time taken: 2.16 seconds.\n",
      "Learning rate: 0.2941. Training accuracy: 0.6913. Validation accuracy: 0.6920. Mean training loss: 1.5902. Mean validation loss: 1.5793. Mean weight gradients: [0.0049, 0.3635, 0.0714].\n",
      "Epoch 18 complete. Time taken: 1.36 seconds.\n",
      "Learning rate: 0.2778. Training accuracy: 0.6997. Validation accuracy: 0.7002. Mean training loss: 1.5792. Mean validation loss: 1.5613. Mean weight gradients: [0.0074, 0.5569, 0.1124].\n",
      "Epoch 19 complete. Time taken: 1.48 seconds.\n",
      "Learning rate: 0.2632. Training accuracy: 0.7070. Validation accuracy: 0.7111. Mean training loss: 1.5658. Mean validation loss: 1.5504. Mean weight gradients: [0.0035, 0.3998, 0.0915].\n",
      "Epoch 20 complete. Time taken: 1.73 seconds.\n",
      "Learning rate: 0.2500. Training accuracy: 0.7154. Validation accuracy: 0.7209. Mean training loss: 1.5518. Mean validation loss: 1.5348. Mean weight gradients: [0.0066, 0.5393, 0.107].\n",
      "Epoch 21 complete. Time taken: 1.65 seconds.\n",
      "Learning rate: 0.2381. Training accuracy: 0.7239. Validation accuracy: 0.7273. Mean training loss: 1.5393. Mean validation loss: 1.5237. Mean weight gradients: [0.0031, 0.2011, 0.0991].\n",
      "Epoch 22 complete. Time taken: 1.38 seconds.\n",
      "Learning rate: 0.2273. Training accuracy: 0.7323. Validation accuracy: 0.7347. Mean training loss: 1.5267. Mean validation loss: 1.5116. Mean weight gradients: [0.005, 0.2611, 0.1132].\n",
      "Epoch 23 complete. Time taken: 1.34 seconds.\n",
      "Learning rate: 0.2174. Training accuracy: 0.7377. Validation accuracy: 0.7421. Mean training loss: 1.5158. Mean validation loss: 1.5034. Mean weight gradients: [0.0042, 0.2977, 0.0946].\n",
      "Epoch 24 complete. Time taken: 1.48 seconds.\n",
      "Learning rate: 0.2083. Training accuracy: 0.7444. Validation accuracy: 0.7479. Mean training loss: 1.5069. Mean validation loss: 1.4892. Mean weight gradients: [0.0048, 0.5432, 0.111].\n",
      "Epoch 25 complete. Time taken: 2.22 seconds.\n",
      "Learning rate: 0.2000. Training accuracy: 0.7481. Validation accuracy: 0.7516. Mean training loss: 1.4988. Mean validation loss: 1.4858. Mean weight gradients: [0.0044, 0.6509, 0.134].\n",
      "Epoch 26 complete. Time taken: 1.83 seconds.\n",
      "Learning rate: 0.1923. Training accuracy: 0.7507. Validation accuracy: 0.7536. Mean training loss: 1.4918. Mean validation loss: 1.4778. Mean weight gradients: [0.004, 0.2471, 0.083].\n",
      "Epoch 27 complete. Time taken: 1.92 seconds.\n",
      "Learning rate: 0.1852. Training accuracy: 0.7530. Validation accuracy: 0.7553. Mean training loss: 1.4856. Mean validation loss: 1.4706. Mean weight gradients: [0.0051, 0.1746, 0.0815].\n",
      "Epoch 28 complete. Time taken: 1.86 seconds.\n",
      "Learning rate: 0.1786. Training accuracy: 0.7552. Validation accuracy: 0.7567. Mean training loss: 1.4796. Mean validation loss: 1.4636. Mean weight gradients: [0.0053, 0.4659, 0.1058].\n",
      "Epoch 29 complete. Time taken: 1.86 seconds.\n",
      "Learning rate: 0.1724. Training accuracy: 0.7565. Validation accuracy: 0.7581. Mean training loss: 1.4742. Mean validation loss: 1.4595. Mean weight gradients: [0.0036, 0.3088, 0.1053].\n",
      "Epoch 30 complete. Time taken: 1.78 seconds.\n",
      "Learning rate: 0.1667. Training accuracy: 0.7572. Validation accuracy: 0.7587. Mean training loss: 1.4692. Mean validation loss: 1.4554. Mean weight gradients: [0.0049, 0.4375, 0.1543].\n",
      "Epoch 31 complete. Time taken: 1.77 seconds.\n",
      "Learning rate: 0.1613. Training accuracy: 0.7581. Validation accuracy: 0.7619. Mean training loss: 1.4637. Mean validation loss: 1.4472. Mean weight gradients: [0.0052, 0.4165, 0.1119].\n",
      "Epoch 32 complete. Time taken: 1.65 seconds.\n",
      "Learning rate: 0.1562. Training accuracy: 0.7603. Validation accuracy: 0.7620. Mean training loss: 1.4596. Mean validation loss: 1.4437. Mean weight gradients: [0.005, 0.3172, 0.0855].\n",
      "Epoch 33 complete. Time taken: 1.60 seconds.\n",
      "Learning rate: 0.1515. Training accuracy: 0.7609. Validation accuracy: 0.7637. Mean training loss: 1.4555. Mean validation loss: 1.4420. Mean weight gradients: [0.007, 0.4196, 0.1553].\n",
      "Epoch 34 complete. Time taken: 1.86 seconds.\n",
      "Learning rate: 0.1471. Training accuracy: 0.7617. Validation accuracy: 0.7659. Mean training loss: 1.4518. Mean validation loss: 1.4346. Mean weight gradients: [0.0039, 0.1465, 0.1085].\n",
      "Epoch 35 complete. Time taken: 2.03 seconds.\n",
      "Learning rate: 0.1429. Training accuracy: 0.7625. Validation accuracy: 0.7663. Mean training loss: 1.4485. Mean validation loss: 1.4300. Mean weight gradients: [0.0066, 0.2463, 0.1463].\n",
      "Epoch 36 complete. Time taken: 1.86 seconds.\n",
      "Learning rate: 0.1389. Training accuracy: 0.7639. Validation accuracy: 0.7671. Mean training loss: 1.4434. Mean validation loss: 1.4274. Mean weight gradients: [0.0048, 0.1991, 0.157].\n",
      "Epoch 37 complete. Time taken: 1.84 seconds.\n",
      "Learning rate: 0.1351. Training accuracy: 0.7646. Validation accuracy: 0.7679. Mean training loss: 1.4404. Mean validation loss: 1.4237. Mean weight gradients: [0.0051, 0.3096, 0.1321].\n",
      "Epoch 38 complete. Time taken: 1.49 seconds.\n",
      "Learning rate: 0.1316. Training accuracy: 0.7646. Validation accuracy: 0.7693. Mean training loss: 1.4382. Mean validation loss: 1.4194. Mean weight gradients: [0.007, 0.3725, 0.1358].\n",
      "Epoch 39 complete. Time taken: 1.65 seconds.\n",
      "Learning rate: 0.1282. Training accuracy: 0.7656. Validation accuracy: 0.7700. Mean training loss: 1.4338. Mean validation loss: 1.4193. Mean weight gradients: [0.0072, 0.4875, 0.0858].\n",
      "Epoch 40 complete. Time taken: 1.74 seconds.\n",
      "Learning rate: 0.1250. Training accuracy: 0.7656. Validation accuracy: 0.7695. Mean training loss: 1.4332. Mean validation loss: 1.4153. Mean weight gradients: [0.0055, 0.3239, 0.0873].\n",
      "Epoch 41 complete. Time taken: 1.61 seconds.\n",
      "Learning rate: 0.1220. Training accuracy: 0.7674. Validation accuracy: 0.7717. Mean training loss: 1.4294. Mean validation loss: 1.4127. Mean weight gradients: [0.0041, 0.3514, 0.2036].\n",
      "Epoch 42 complete. Time taken: 1.66 seconds.\n",
      "Learning rate: 0.1190. Training accuracy: 0.7673. Validation accuracy: 0.7711. Mean training loss: 1.4276. Mean validation loss: 1.4099. Mean weight gradients: [0.0038, 0.1675, 0.1251].\n",
      "Epoch 43 complete. Time taken: 1.75 seconds.\n",
      "Learning rate: 0.1163. Training accuracy: 0.7677. Validation accuracy: 0.7713. Mean training loss: 1.4263. Mean validation loss: 1.4106. Mean weight gradients: [0.0053, 0.2067, 0.1538].\n",
      "Epoch 44 complete. Time taken: 1.66 seconds.\n",
      "Learning rate: 0.1136. Training accuracy: 0.7677. Validation accuracy: 0.7722. Mean training loss: 1.4242. Mean validation loss: 1.4035. Mean weight gradients: [0.0047, 0.2346, 0.0927].\n",
      "Epoch 45 complete. Time taken: 1.88 seconds.\n",
      "Learning rate: 0.1111. Training accuracy: 0.7689. Validation accuracy: 0.7726. Mean training loss: 1.4229. Mean validation loss: 1.4014. Mean weight gradients: [0.0065, 0.191, 0.1038].\n",
      "Epoch 46 complete. Time taken: 1.69 seconds.\n",
      "Learning rate: 0.1087. Training accuracy: 0.7697. Validation accuracy: 0.7720. Mean training loss: 1.4190. Mean validation loss: 1.4049. Mean weight gradients: [0.0046, 0.3227, 0.1264].\n",
      "Epoch 47 complete. Time taken: 1.70 seconds.\n",
      "Learning rate: 0.1064. Training accuracy: 0.7693. Validation accuracy: 0.7729. Mean training loss: 1.4187. Mean validation loss: 1.4012. Mean weight gradients: [0.0044, 0.1522, 0.1132].\n",
      "Epoch 48 complete. Time taken: 2.06 seconds.\n",
      "Learning rate: 0.1042. Training accuracy: 0.7701. Validation accuracy: 0.7725. Mean training loss: 1.4162. Mean validation loss: 1.4026. Mean weight gradients: [0.0054, 0.2481, 0.1515].\n",
      "Training complete.\n",
      "Test accuracy: 0.7633. Mean test loss: 1.4403525504740369\n"
     ]
    }
   ],
   "source": [
    "# Build & train first model\n",
    "model1 = Model(sizes, weights, biases)\n",
    "model1.fit(train_data, epochs = epochs, batch_size = batch_size, learning_rate = learning_rate, decay_strength = decay_strength, validation_size = validation_size, shuffle_order = shuffle_order, store_latent_vecs = False)\n",
    "test_acc, test_loss = model1.evaluate(test_data)\n",
    "print(f'Test accuracy: {test_acc}. Mean test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model parameters: 23642\n",
      "Baseline characteristics: Training accuracy: 0.0991. Validation accuracy: 0.0969. Training loss: 4.5946. Validation loss: 4.5910\n",
      "Training on 42000 examples, validating on 12600 examples\n",
      "Epoch 1 complete. Time taken: 2.54 seconds.\n",
      "Learning rate: 5.0000. Training accuracy: 0.3160. Validation accuracy: 0.4110. Mean training loss: 2.7026. Mean validation loss: 2.2397. Mean weight gradients: [0.0039, 0.0815, 0.0642].\n",
      "Epoch 2 complete. Time taken: 3.62 seconds.\n",
      "Learning rate: 2.5000. Training accuracy: 0.4813. Validation accuracy: 0.5067. Mean training loss: 2.1382. Mean validation loss: 2.0499. Mean weight gradients: [0.0049, 0.181, 0.1341].\n",
      "Epoch 3 complete. Time taken: 1.54 seconds.\n",
      "Learning rate: 1.6667. Training accuracy: 0.5492. Validation accuracy: 0.5687. Mean training loss: 1.9916. Mean validation loss: 1.9299. Mean weight gradients: [0.0029, 0.2282, 0.1493].\n",
      "Epoch 4 complete. Time taken: 1.77 seconds.\n",
      "Learning rate: 1.2500. Training accuracy: 0.5813. Validation accuracy: 0.5817. Mean training loss: 1.8976. Mean validation loss: 1.8703. Mean weight gradients: [0.0036, 0.1547, 0.1381].\n",
      "Epoch 5 complete. Time taken: 2.34 seconds.\n",
      "Learning rate: 1.0000. Training accuracy: 0.5982. Validation accuracy: 0.6089. Mean training loss: 1.8311. Mean validation loss: 1.8008. Mean weight gradients: [0.0045, 0.2455, 0.1349].\n",
      "Epoch 6 complete. Time taken: 1.60 seconds.\n",
      "Learning rate: 0.8333. Training accuracy: 0.6127. Validation accuracy: 0.6119. Mean training loss: 1.7834. Mean validation loss: 1.7514. Mean weight gradients: [0.0036, 0.2358, 0.0583].\n",
      "Epoch 7 complete. Time taken: 4.74 seconds.\n",
      "Learning rate: 0.7143. Training accuracy: 0.6210. Validation accuracy: 0.6253. Mean training loss: 1.7468. Mean validation loss: 1.7260. Mean weight gradients: [0.0045, 0.1584, 0.1104].\n",
      "Epoch 8 complete. Time taken: 2.22 seconds.\n",
      "Learning rate: 0.6250. Training accuracy: 0.6300. Validation accuracy: 0.6346. Mean training loss: 1.7202. Mean validation loss: 1.7068. Mean weight gradients: [0.0047, 0.2665, 0.1028].\n",
      "Epoch 9 complete. Time taken: 1.67 seconds.\n",
      "Learning rate: 0.5556. Training accuracy: 0.6349. Validation accuracy: 0.6403. Mean training loss: 1.6981. Mean validation loss: 1.6833. Mean weight gradients: [0.0054, 0.1828, 0.0654].\n",
      "Epoch 10 complete. Time taken: 2.68 seconds.\n",
      "Learning rate: 0.5000. Training accuracy: 0.6426. Validation accuracy: 0.6460. Mean training loss: 1.6803. Mean validation loss: 1.6666. Mean weight gradients: [0.0067, 0.1876, 0.0485].\n",
      "Epoch 11 complete. Time taken: 2.90 seconds.\n",
      "Learning rate: 0.4545. Training accuracy: 0.6473. Validation accuracy: 0.6511. Mean training loss: 1.6650. Mean validation loss: 1.6529. Mean weight gradients: [0.0034, 0.3032, 0.076].\n",
      "Epoch 12 complete. Time taken: 2.38 seconds.\n",
      "Learning rate: 0.4167. Training accuracy: 0.6542. Validation accuracy: 0.6574. Mean training loss: 1.6502. Mean validation loss: 1.6385. Mean weight gradients: [0.0062, 0.4481, 0.0884].\n",
      "Epoch 13 complete. Time taken: 3.02 seconds.\n",
      "Learning rate: 0.3846. Training accuracy: 0.6622. Validation accuracy: 0.6533. Mean training loss: 1.6371. Mean validation loss: 1.6276. Mean weight gradients: [0.007, 0.3162, 0.0867].\n",
      "Epoch 14 complete. Time taken: 2.21 seconds.\n",
      "Learning rate: 0.3571. Training accuracy: 0.6655. Validation accuracy: 0.6690. Mean training loss: 1.6265. Mean validation loss: 1.6115. Mean weight gradients: [0.0045, 0.266, 0.0912].\n",
      "Epoch 15 complete. Time taken: 3.23 seconds.\n",
      "Learning rate: 0.3333. Training accuracy: 0.6747. Validation accuracy: 0.6734. Mean training loss: 1.6146. Mean validation loss: 1.6022. Mean weight gradients: [0.0052, 0.311, 0.1053].\n",
      "Epoch 16 complete. Time taken: 1.57 seconds.\n",
      "Learning rate: 0.3125. Training accuracy: 0.6831. Validation accuracy: 0.6826. Mean training loss: 1.6034. Mean validation loss: 1.5906. Mean weight gradients: [0.003, 0.2433, 0.0635].\n",
      "Epoch 17 complete. Time taken: 1.81 seconds.\n",
      "Learning rate: 0.2941. Training accuracy: 0.6913. Validation accuracy: 0.6920. Mean training loss: 1.5902. Mean validation loss: 1.5793. Mean weight gradients: [0.0049, 0.3635, 0.0714].\n",
      "Epoch 18 complete. Time taken: 1.72 seconds.\n",
      "Learning rate: 0.2778. Training accuracy: 0.6997. Validation accuracy: 0.7002. Mean training loss: 1.5792. Mean validation loss: 1.5613. Mean weight gradients: [0.0074, 0.5569, 0.1124].\n",
      "Epoch 19 complete. Time taken: 1.85 seconds.\n",
      "Learning rate: 0.2632. Training accuracy: 0.7070. Validation accuracy: 0.7111. Mean training loss: 1.5658. Mean validation loss: 1.5504. Mean weight gradients: [0.0035, 0.3998, 0.0915].\n",
      "Epoch 20 complete. Time taken: 1.77 seconds.\n",
      "Learning rate: 0.2500. Training accuracy: 0.7154. Validation accuracy: 0.7209. Mean training loss: 1.5518. Mean validation loss: 1.5348. Mean weight gradients: [0.0066, 0.5393, 0.107].\n",
      "Epoch 21 complete. Time taken: 1.72 seconds.\n",
      "Learning rate: 0.2381. Training accuracy: 0.7239. Validation accuracy: 0.7273. Mean training loss: 1.5393. Mean validation loss: 1.5237. Mean weight gradients: [0.0031, 0.2011, 0.0991].\n",
      "Epoch 22 complete. Time taken: 1.79 seconds.\n",
      "Learning rate: 0.2273. Training accuracy: 0.7323. Validation accuracy: 0.7347. Mean training loss: 1.5267. Mean validation loss: 1.5116. Mean weight gradients: [0.005, 0.2611, 0.1132].\n",
      "Epoch 23 complete. Time taken: 2.07 seconds.\n",
      "Learning rate: 0.2174. Training accuracy: 0.7377. Validation accuracy: 0.7421. Mean training loss: 1.5158. Mean validation loss: 1.5034. Mean weight gradients: [0.0042, 0.2977, 0.0946].\n",
      "Epoch 24 complete. Time taken: 1.68 seconds.\n",
      "Learning rate: 0.2083. Training accuracy: 0.7444. Validation accuracy: 0.7479. Mean training loss: 1.5069. Mean validation loss: 1.4892. Mean weight gradients: [0.0048, 0.5432, 0.111].\n",
      "Epoch 25 complete. Time taken: 1.64 seconds.\n",
      "Learning rate: 0.2000. Training accuracy: 0.7481. Validation accuracy: 0.7516. Mean training loss: 1.4988. Mean validation loss: 1.4858. Mean weight gradients: [0.0044, 0.6509, 0.134].\n",
      "Epoch 26 complete. Time taken: 1.96 seconds.\n",
      "Learning rate: 0.1923. Training accuracy: 0.7507. Validation accuracy: 0.7536. Mean training loss: 1.4918. Mean validation loss: 1.4778. Mean weight gradients: [0.004, 0.2471, 0.083].\n",
      "Epoch 27 complete. Time taken: 1.58 seconds.\n",
      "Learning rate: 0.1852. Training accuracy: 0.7530. Validation accuracy: 0.7553. Mean training loss: 1.4856. Mean validation loss: 1.4706. Mean weight gradients: [0.0051, 0.1746, 0.0815].\n",
      "Epoch 28 complete. Time taken: 1.80 seconds.\n",
      "Learning rate: 0.1786. Training accuracy: 0.7552. Validation accuracy: 0.7567. Mean training loss: 1.4796. Mean validation loss: 1.4636. Mean weight gradients: [0.0053, 0.4659, 0.1058].\n",
      "Epoch 29 complete. Time taken: 1.71 seconds.\n",
      "Learning rate: 0.1724. Training accuracy: 0.7565. Validation accuracy: 0.7581. Mean training loss: 1.4742. Mean validation loss: 1.4595. Mean weight gradients: [0.0036, 0.3088, 0.1053].\n",
      "Epoch 30 complete. Time taken: 1.75 seconds.\n",
      "Learning rate: 0.1667. Training accuracy: 0.7572. Validation accuracy: 0.7587. Mean training loss: 1.4692. Mean validation loss: 1.4554. Mean weight gradients: [0.0049, 0.4375, 0.1543].\n",
      "Epoch 31 complete. Time taken: 1.89 seconds.\n",
      "Learning rate: 0.1613. Training accuracy: 0.7581. Validation accuracy: 0.7619. Mean training loss: 1.4637. Mean validation loss: 1.4472. Mean weight gradients: [0.0052, 0.4165, 0.1119].\n",
      "Epoch 32 complete. Time taken: 1.73 seconds.\n",
      "Learning rate: 0.1562. Training accuracy: 0.7603. Validation accuracy: 0.7620. Mean training loss: 1.4596. Mean validation loss: 1.4437. Mean weight gradients: [0.005, 0.3172, 0.0855].\n",
      "Epoch 33 complete. Time taken: 1.67 seconds.\n",
      "Learning rate: 0.1515. Training accuracy: 0.7609. Validation accuracy: 0.7637. Mean training loss: 1.4555. Mean validation loss: 1.4420. Mean weight gradients: [0.007, 0.4196, 0.1553].\n",
      "Epoch 34 complete. Time taken: 1.84 seconds.\n",
      "Learning rate: 0.1471. Training accuracy: 0.7617. Validation accuracy: 0.7659. Mean training loss: 1.4518. Mean validation loss: 1.4346. Mean weight gradients: [0.0039, 0.1465, 0.1085].\n",
      "Epoch 35 complete. Time taken: 1.58 seconds.\n",
      "Learning rate: 0.1429. Training accuracy: 0.7625. Validation accuracy: 0.7663. Mean training loss: 1.4485. Mean validation loss: 1.4300. Mean weight gradients: [0.0066, 0.2463, 0.1463].\n",
      "Epoch 36 complete. Time taken: 1.73 seconds.\n",
      "Learning rate: 0.1389. Training accuracy: 0.7639. Validation accuracy: 0.7671. Mean training loss: 1.4434. Mean validation loss: 1.4274. Mean weight gradients: [0.0048, 0.1991, 0.157].\n",
      "Epoch 37 complete. Time taken: 1.86 seconds.\n",
      "Learning rate: 0.1351. Training accuracy: 0.7646. Validation accuracy: 0.7679. Mean training loss: 1.4404. Mean validation loss: 1.4237. Mean weight gradients: [0.0051, 0.3096, 0.1321].\n",
      "Epoch 38 complete. Time taken: 1.78 seconds.\n",
      "Learning rate: 0.1316. Training accuracy: 0.7646. Validation accuracy: 0.7693. Mean training loss: 1.4382. Mean validation loss: 1.4194. Mean weight gradients: [0.007, 0.3725, 0.1358].\n",
      "Epoch 39 complete. Time taken: 1.74 seconds.\n",
      "Learning rate: 0.1282. Training accuracy: 0.7656. Validation accuracy: 0.7700. Mean training loss: 1.4338. Mean validation loss: 1.4193. Mean weight gradients: [0.0072, 0.4875, 0.0858].\n",
      "Epoch 40 complete. Time taken: 1.93 seconds.\n",
      "Learning rate: 0.1250. Training accuracy: 0.7656. Validation accuracy: 0.7695. Mean training loss: 1.4332. Mean validation loss: 1.4153. Mean weight gradients: [0.0055, 0.3239, 0.0873].\n",
      "Epoch 41 complete. Time taken: 1.82 seconds.\n",
      "Learning rate: 0.1220. Training accuracy: 0.7674. Validation accuracy: 0.7717. Mean training loss: 1.4294. Mean validation loss: 1.4127. Mean weight gradients: [0.0041, 0.3514, 0.2036].\n",
      "Epoch 42 complete. Time taken: 2.04 seconds.\n",
      "Learning rate: 0.1190. Training accuracy: 0.7673. Validation accuracy: 0.7711. Mean training loss: 1.4276. Mean validation loss: 1.4099. Mean weight gradients: [0.0038, 0.1675, 0.1251].\n",
      "Epoch 43 complete. Time taken: 1.93 seconds.\n",
      "Learning rate: 0.1163. Training accuracy: 0.7677. Validation accuracy: 0.7713. Mean training loss: 1.4263. Mean validation loss: 1.4106. Mean weight gradients: [0.0053, 0.2067, 0.1538].\n",
      "Epoch 44 complete. Time taken: 2.33 seconds.\n",
      "Learning rate: 0.1136. Training accuracy: 0.7677. Validation accuracy: 0.7722. Mean training loss: 1.4242. Mean validation loss: 1.4035. Mean weight gradients: [0.0047, 0.2346, 0.0927].\n",
      "Epoch 45 complete. Time taken: 3.80 seconds.\n",
      "Learning rate: 0.1111. Training accuracy: 0.7689. Validation accuracy: 0.7726. Mean training loss: 1.4229. Mean validation loss: 1.4014. Mean weight gradients: [0.0065, 0.191, 0.1038].\n",
      "Epoch 46 complete. Time taken: 2.87 seconds.\n",
      "Learning rate: 0.1087. Training accuracy: 0.7697. Validation accuracy: 0.7720. Mean training loss: 1.4190. Mean validation loss: 1.4049. Mean weight gradients: [0.0046, 0.3227, 0.1264].\n",
      "Epoch 47 complete. Time taken: 1.99 seconds.\n",
      "Learning rate: 0.1064. Training accuracy: 0.7693. Validation accuracy: 0.7729. Mean training loss: 1.4187. Mean validation loss: 1.4012. Mean weight gradients: [0.0044, 0.1522, 0.1132].\n",
      "Epoch 48 complete. Time taken: 2.04 seconds.\n",
      "Learning rate: 0.1042. Training accuracy: 0.7701. Validation accuracy: 0.7725. Mean training loss: 1.4162. Mean validation loss: 1.4026. Mean weight gradients: [0.0054, 0.2481, 0.1515].\n",
      "Training complete.\n",
      "Test accuracy: 0.7633. Mean test loss: 1.4403525504740369\n"
     ]
    }
   ],
   "source": [
    "# Build & train second model\n",
    "model2 = Model(sizes, weights, biases)\n",
    "model2.fit(train_data, epochs = epochs, batch_size = batch_size, learning_rate = learning_rate, validation_size = validation_size, shuffle_order = shuffle_order, store_latent_vecs = True)\n",
    "test_acc, test_loss = model2.evaluate(test_data)\n",
    "print(f'Test accuracy: {test_acc}. Mean test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights were reproducible: [True, True, True]\n"
     ]
    }
   ],
   "source": [
    "# Are model weights reproducible?\n",
    "print(f'Model weights were reproducible: {[np.array_equal(model1.weights[i], model2.weights[i]) for i in range(len(model1.weights))]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000000000000001\n",
      "[[7.53357724e-01]\n",
      " [9.39750017e-07]\n",
      " [9.60527529e-02]\n",
      " [7.91975256e-02]\n",
      " [1.70967292e-07]\n",
      " [7.05581190e-02]\n",
      " [4.09318331e-04]\n",
      " [1.22381115e-07]\n",
      " [4.23176822e-04]\n",
      " [1.49785695e-07]]\n",
      "[[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# Inspect last layer activations\n",
    "last_layer = [model1.feedforward(x, layer_num = 3) for x, y in train_data[:1]]\n",
    "print(last_layer[0].sum())\n",
    "print(last_layer[0])\n",
    "print(train_data[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1. Visualize latent space over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('results/p1_model.pkl'):\n",
    "    # Build & train model\n",
    "    model = Model(sizes, weights, biases)\n",
    "    model.fit(train_data, epochs = epochs, batch_size = batch_size, learning_rate = learning_rate, validation_size = validation_size, shuffle_order = shuffle_order, store_latent_vecs = True)\n",
    "    test_acc, test_loss = model.evaluate(test_data)\n",
    "    print(f'Test accuracy: {test_acc}. Mean test loss: {test_loss}')\n",
    "\n",
    "    # Save model to file\n",
    "    with open('results/p1_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "else:\n",
    "    # Read model from file\n",
    "    with open('results/p1_model.pkl', 'rb') as f:\n",
    "        model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('results/p1_history.png'):\n",
    "    # Plot model history\n",
    "    fig, ax = plt.subplots(1, 2, figsize = (14, 5))\n",
    "\n",
    "    # Plot accuracy history\n",
    "    ax[0].plot(range(len(model.history['val']['acc'])), model.history['val']['acc'], label='Validation')\n",
    "    ax[0].plot(range(len(model.history['train']['acc'])), model.history['train']['acc'], label='Train')\n",
    "    ax[0].set_xticks(np.arange(0, len(model.history['val']['acc']), 2)) # Set xticks\n",
    "    ax[0].set_ylabel('Accuracy') # Y axis label\n",
    "    ax[0].set_xlabel('Epochs') # X axis label\n",
    "    ax[0].legend() # Add legend\n",
    "\n",
    "    # Plot loss history\n",
    "    ax[1].plot(range(len(model.history['val']['loss'])), model.history['val']['loss'], label='Validation')\n",
    "    ax[1].plot(range(len(model.history['train']['loss'])), model.history['train']['loss'], label='Train')\n",
    "    ax[1].set_xticks(np.arange(0, len(model.history['val']['loss']), 2)) # Set xticks\n",
    "    ax[1].set_ylabel('Loss') # Y axis label\n",
    "    ax[1].set_xlabel('Epochs') # X axis label\n",
    "    ax[1].legend() # Add legend\n",
    "\n",
    "    # Save plot to file\n",
    "    plt.savefig(f'results/p1_history.png')\n",
    "    plt.show();\n",
    "else:\n",
    "    # Load plot from file\n",
    "    plt.figure(figsize = (14, 5))\n",
    "    plt.axes([0,0,1,1])\n",
    "    plt.imshow(mpimg.imread('results/p1_history.png'))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('results/p1_latent_over_epochs.png'):\n",
    "    # Retrieve latent vectors from model history and plot them\n",
    "    nrows = 7\n",
    "    ncols = 7\n",
    "    fig, ax = plt.subplots(nrows, ncols, tight_layout=True, figsize = (ncols * 5, nrows * 5))\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    colors = sns.color_palette(['#e6194B','#f58231','#ffe119','#bfef45','#3cb44b','#42d4f4','#4363d8', '#000075', '#911eb4','#f032e6'])\n",
    "\n",
    "    for epoch in range(0, len(model.history['latent'])):\n",
    "        # Create dataframe\n",
    "        df = []\n",
    "        for index, element in enumerate(model.history['latent'][epoch]):\n",
    "            vec, label = element\n",
    "            df.append({'x': vec[0][0], 'y': vec[1][0], 'digit': label})\n",
    "        df = pd.DataFrame(df)\n",
    "\n",
    "        # Plot latent vectors in 2D, color-coded by digit\n",
    "        # print(f'Plotting {len(df)} examples.')\n",
    "        sns.scatterplot(data=df, x=\"x\", y=\"y\", hue=\"digit\", palette=colors, ax=ax[epoch], legend=False)\n",
    "\n",
    "        # Add title\n",
    "        if epoch == 0:\n",
    "            title = 'Before training'\n",
    "        else:\n",
    "            title = f'After epoch {epoch}'\n",
    "        ax[epoch].set_title(title)\n",
    "\n",
    "    # Create legend in last subplot\n",
    "    for i in range(10):\n",
    "        ax[-1].scatter([], [], color=colors[i], label=i)\n",
    "    ax[-1].legend()\n",
    "\n",
    "    # Save plot to file\n",
    "    plt.savefig(f'results/p1_latent_over_epochs.png')\n",
    "    plt.show();\n",
    "else:\n",
    "    # Load plot from file\n",
    "    plt.figure(figsize = (35, 35))\n",
    "    plt.axes([0,0,1,1])\n",
    "    plt.imshow(mpimg.imread('results/p1_latent_over_epochs.png'))\n",
    "    plt.axis('off')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantify latent space similarity versus training loss and model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get adjacency matrix for a given latent distribution\n",
    "def get_adjacency_matrix(latent_distribution):\n",
    "    # Create dataframe\n",
    "    df = []\n",
    "    for index, element in enumerate(latent_distribution):\n",
    "        vec, label = element\n",
    "        df.append({'x': vec[0][0], 'y': vec[1][0], 'digit': label})\n",
    "    df = pd.DataFrame(df)\n",
    "\n",
    "    # Get centroid coordinates\n",
    "    centroids = df.groupby('digit').mean()\n",
    "\n",
    "    # Create adjacency matrix\n",
    "    adjacency_matrix = np.zeros((10, 10))\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            if i == j:\n",
    "                continue\n",
    "            else:\n",
    "                adjacency_matrix[i][j] = np.linalg.norm(centroids.iloc[i] - centroids.iloc[j]) # Euclidean distance (L2 norm)\n",
    "    \n",
    "    return adjacency_matrix\n",
    "\n",
    "# Function to compute adjacency spectral distance\n",
    "def adjacency_spectral_distance(adjacency_matrix_1, adjacency_matrix_2):\n",
    "    # Compute spectral distance\n",
    "    eigvals_1, eigvecs_1 = np.linalg.eig(adjacency_matrix_1)\n",
    "    eigvals_2, eigvecs_2 = np.linalg.eig(adjacency_matrix_2)\n",
    "    spectral_distance = np.linalg.norm(eigvals_1 - eigvals_2)\n",
    "\n",
    "    return spectral_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('results/p1_adjacency_mean_v_loss.png'):\n",
    "    # Mean of adjacency matrix vs. training loss\n",
    "    # Calculate adjacency matrix mean for each epoch, including before training\n",
    "    adjacency_matrix_means = []\n",
    "    for epoch in range(0, len(model.history['latent'])):\n",
    "        adjacency_matrix_means.append(np.mean(get_adjacency_matrix(model.history['latent'][epoch])))\n",
    "\n",
    "    # Get list of training loss values\n",
    "    train_losses = model.history['train']['loss']\n",
    "\n",
    "    # Plot adjacency mean vs. training loss\n",
    "    # Color by epoch\n",
    "    plt.scatter(adjacency_matrix_means, model.history['train']['loss'], c=range(len(model.history['train']['loss'])))\n",
    "    plt.xlabel('Adjacency mean')\n",
    "    plt.ylabel('Training loss')\n",
    "    plt.title('Adjacency mean vs. training loss')\n",
    "\n",
    "    # Save plot to file\n",
    "    plt.savefig(f'results/p1_adjacency_mean_v_loss.png')\n",
    "    plt.show();\n",
    "else:\n",
    "    # Load plot from file\n",
    "    plt.axes([0,0,1,1])\n",
    "    plt.imshow(mpimg.imread('results/p1_adjacency_mean_v_loss.png'))\n",
    "    plt.axis('off')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('results/p1_cluster_sep_over_epochs.png'):\n",
    "    # Plot separation between clusters over epochs for all pairs of digits\n",
    "    nrows = 2\n",
    "    ncols = 5\n",
    "    fig, ax = plt.subplots(nrows, ncols, tight_layout=True, figsize = (ncols * 5, nrows * 5))\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    colors = sns.color_palette(['#e6194B','#f58231','#ffe119','#bfef45','#3cb44b','#42d4f4','#4363d8', '#000075', '#911eb4','#f032e6'])\n",
    "\n",
    "    for digit1 in range(10):\n",
    "        dict_of_distances = {}\n",
    "        for digit2 in range(10):\n",
    "            distances = []\n",
    "            for epoch in range(0, len(model.history['latent'])):\n",
    "                # Get separation between digit1 and digit2 cluster centroids from adjacency matrix\n",
    "                distances.append(get_adjacency_matrix(model.history['latent'][epoch])[digit1][digit2])\n",
    "            dict_of_distances[digit2] = distances\n",
    "        df = pd.DataFrame(dict_of_distances)\n",
    "\n",
    "        # Plot separation between clusters over epochs for all pairs of digits\n",
    "        sns.lineplot(data=df, ax=ax[digit1], legend=True)\n",
    "        ax[digit1].set_title(f'Separation from digit {digit1} over epochs')\n",
    "        ax[digit1].set_xlabel('Epoch')\n",
    "        ax[digit1].set_ylabel('Separation')\n",
    "        ax[digit1].set_xticks(np.arange(0, len(model.history['val']['loss']), 4)) # Set xticks\n",
    "    \n",
    "    # Add title\n",
    "    title = 'Separation between clusters over epochs for all pairs of digits'\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    # Save plot to file\n",
    "    plt.savefig(f'results/p1_cluster_sep_over_epochs.png')\n",
    "    plt.show();\n",
    "else:\n",
    "    # Load plot from file\n",
    "    plt.axes([0,0,1,1])\n",
    "    plt.imshow(mpimg.imread('results/p1_cluster_sep_over_epochs.png'))\n",
    "    plt.axis('off')\n",
    "    plt.show();            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.path.exists('results/p1_misclassification_rate.png'):\n",
    "#     # Plot bar graph of misclassification rates for each class\n",
    "#     # Get list of misclassification rates\n",
    "# else:\n",
    "#     # Load plot from file\n",
    "#     plt.axes([0,0,1,1])\n",
    "#     plt.imshow(mpimg.imread('results/p1_misclassification_rate.png'))\n",
    "#     plt.axis('off')\n",
    "#     plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('results/p1_spectral_distance_v_loss_delta.png'):\n",
    "    # Adjacency spectral distance vs. change in training loss for consecutive epochs\n",
    "    # Calculate adjacency spectral distance between latent distributions of consecutive epochs (starting from epoch 1)\n",
    "    spectral_distances = []\n",
    "    for epoch in range(1, len(model.history['latent']) - 1):\n",
    "        adjacency_matrix_1 = get_adjacency_matrix(model.history['latent'][epoch])\n",
    "        adjacency_matrix_2 = get_adjacency_matrix(model.history['latent'][epoch + 1])\n",
    "        spectral_distances.append(adjacency_spectral_distance(adjacency_matrix_1, adjacency_matrix_2))\n",
    "\n",
    "    # Get change in training loss between consecutive epochs (starting from epoch 1)\n",
    "    loss_deltas = []\n",
    "    for epoch in range(1, len(model.history['train']['loss']) - 1):\n",
    "        loss_deltas.append(model.history['train']['loss'][epoch + 1] - model.history['train']['loss'][epoch])\n",
    "\n",
    "    # Plot spectral distance vs. change in training loss\n",
    "    # Color by epoch\n",
    "    plt.scatter(spectral_distances, loss_deltas, c=range(1, len(model.history['train']['loss']) - 1))\n",
    "    plt.xlabel('Spectral distance')\n",
    "    plt.ylabel('Change in training loss')\n",
    "    plt.title('Spectral distance vs. change in training loss')\n",
    "\n",
    "    # Save plot to file\n",
    "    plt.savefig(f'results/p1_spectral_distance_v_loss_delta.png', bbox_inches='tight')\n",
    "    plt.show();\n",
    "else:\n",
    "    # Load plot from file\n",
    "    plt.axes([0,0,1,1])\n",
    "    plt.imshow(mpimg.imread('results/p1_spectral_distance_v_loss_delta.png'))\n",
    "    plt.axis('off')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('results/p1_spectral_distance_v_weight_distance.png'):\n",
    "    # Adjacency spectral distance vs. Frobenius weights distance for consecutive epochs\n",
    "    # Get Frobenius distance between bottleneck weight matrices of consecutive epochs\n",
    "    weight_distances = []\n",
    "    for i in range(1, len(model.history['weights']) - 1):\n",
    "        weight_distances.append(np.linalg.norm(model.history['weights'][i + 1][-2] - model.history['weights'][i][-2])) # Bottleneck layer is second to last layer\n",
    "\n",
    "    # Plot spectral distance vs. Frobenius weights distance\n",
    "    plt.scatter(spectral_distances, weight_distances, c=range(1, len(model.history['train']['loss']) - 1))\n",
    "    plt.xlabel('Spectral distance')\n",
    "    plt.ylabel('Frobenius weights distance')\n",
    "    plt.title('Spectral distance vs. Frobenius weights distance')\n",
    "\n",
    "    # Save plot to file\n",
    "    plt.savefig(f'results/p1_spectral_distance_v_weight_distance.png')\n",
    "    plt.show();\n",
    "else:\n",
    "    # Load plot from file\n",
    "    plt.axes([0,0,1,1])\n",
    "    plt.imshow(mpimg.imread('results/p1_spectral_distance_v_weight_distance.png'))\n",
    "    plt.axis('off')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2. Investigate effects of weight initialization on latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "sizes = [784, 30, 2, 10]\n",
    "epochs = 48\n",
    "batch_size = 10\n",
    "learning_rate = 1.5\n",
    "validation_size=0.3\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Initialize weights array of shape (num_neurons_i, num_neurons_i-1) for each layer i except the input layer\n",
    "weights = [np.random.randn(y, x) for y, x in zip(sizes[1:], sizes[:-1])]\n",
    "\n",
    "# Initialize biases array of shape (num_neurons_i, 1) for each layer i except the input layer\n",
    "biases = [np.random.randn(y, 1) for y in sizes[1:]] \n",
    "\n",
    "# Initialize list of data indices for data shuffling per epoch\n",
    "shuffle_order = [np.random.permutation(int(len(train_data)*(1 - validation_size))) for i in range(epochs)]\n",
    "\n",
    "if not os.path.exists('results/p2_models_list.pkl'):\n",
    "    # Train 20 models with different initial weights of first neuron for first pixel in first hidden layer\n",
    "    models_list = []\n",
    "    for idx, variable_weight in enumerate(np.linspace(-2, 2, 20)):\n",
    "        # Edit initial weight matrix\n",
    "        new_weights = weights.copy()\n",
    "        new_weights[0][0][0] = variable_weight\n",
    "        \n",
    "        # Build & train model\n",
    "        print(f'Building model {idx} with initial weight[0][0][0]: {new_weights[0][0][0]:.4f}...')\n",
    "        model = Model(sizes, weights=new_weights, biases=biases)\n",
    "        model.fit(train_data, epochs = epochs, batch_size = batch_size, learning_rate = learning_rate, validation_size = validation_size, shuffle_order = shuffle_order, store_latent_vecs = True, verbose=2)\n",
    "        models_list.append(deepcopy(model)) # Must deepcopy model to avoid overwriting previous models\n",
    "\n",
    "    # Save models_list to file\n",
    "    with open(f'results/p2_models_list.pkl', 'wb') as f:\n",
    "        pickle.dump(models_list, f)\n",
    "else:\n",
    "    # Open models_list from file\n",
    "    with open(f'results/p2_models_list.pkl', 'rb') as f:\n",
    "        models_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('results/p2_latent_over_initweights.png'):\n",
    "    # Visualize final latent space for each model\n",
    "    nrows = 5\n",
    "    ncols = 4\n",
    "    fig, ax = plt.subplots(nrows, ncols, tight_layout=True, figsize = (ncols * 5, nrows * 5))\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    colors = sns.color_palette(['#e6194B','#f58231','#ffe119','#bfef45','#3cb44b','#42d4f4','#4363d8', '#000075', '#911eb4','#f032e6'])\n",
    "\n",
    "    for i, model in enumerate(models_list):\n",
    "        # Get latent vecs for final epoch\n",
    "        latent_vecs = model.history['latent'][-1]\n",
    "        # Create dataframe\n",
    "        df = []\n",
    "        for index, element in enumerate(latent_vecs):\n",
    "            vec, label = element\n",
    "            df.append({'x': vec[0][0], 'y': vec[1][0], 'digit': label})\n",
    "        df = pd.DataFrame(df)\n",
    "\n",
    "        # Plot latent vectors in 2D, color-coded by digit\n",
    "        # print(f'Plotting {len(df)} examples.')\n",
    "        sns.scatterplot(data=df, x=\"x\", y=\"y\", hue=\"digit\", palette=colors, ax=ax[i], legend=False)\n",
    "\n",
    "        # Add title\n",
    "        title = f'Weight[0][0][0] = {model.initial_weights[0][0][0]}'\n",
    "        ax[i].set_title(title)\n",
    "\n",
    "    # Create legend in last subplot\n",
    "    for i in range(10):\n",
    "        ax[-1].scatter([], [], color=colors[i], label=i)\n",
    "    ax[-1].legend()\n",
    "\n",
    "    # Save plot to file\n",
    "    plt.savefig(f'results/p2_latent_over_initweights.png');\n",
    "    plt.show();\n",
    "else:\n",
    "    # Load plot from file\n",
    "    plt.axes([0,0,1,1])\n",
    "    plt.imshow(mpimg.imread('results/p2_latent_over_initweights.png'))\n",
    "    plt.axis('off')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('results/p2_spectral_distance_v_weights_delta.png'):\n",
    "    # Plot latent space similarity (by adjacency spectral distance) versus difference in initial weights, for each pair of models in models_list\n",
    "    spectral_distances = []\n",
    "    weight_deltas = []\n",
    "    for i in range(len(models_list)):\n",
    "        for j in range(i + 1, len(models_list)):\n",
    "            # Get adjacency spectral distances of last epoch latent vectors\n",
    "            adjacency_matrix_1 = get_adjacency_matrix(models_list[i].history['latent'][-1])\n",
    "            adjacency_matrix_2 = get_adjacency_matrix(models_list[j].history['latent'][-1])\n",
    "            spectral_distances.append(adjacency_spectral_distance(adjacency_matrix_1, adjacency_matrix_2))\n",
    "\n",
    "            # Get absolute value difference between initial weights of first neuron for first pixel in first hidden layer\n",
    "            weight_difference = np.abs(models_list[i].initial_weights[0][0][0] - models_list[j].initial_weights[0][0][0])\n",
    "            weight_deltas.append(weight_difference)\n",
    "\n",
    "    # Plot spectral distance vs. change in training loss\n",
    "    # Color by epoch\n",
    "    plt.scatter(spectral_distances, weight_deltas)\n",
    "    plt.xlabel('Spectral distance')\n",
    "    plt.ylabel('Difference in initial weight of first neuron in first hidden layer')\n",
    "    plt.title('Spectral distance vs. difference in initial weight of first neuron in first hidden layer')\n",
    "\n",
    "    # Save plot to file\n",
    "    plt.savefig(f'results/p2_spectral_distance_v_weights_delta.png', bbox_inches='tight')\n",
    "    plt.show();\n",
    "else:\n",
    "    # Load plot from file\n",
    "    plt.axes([0,0,1,1])\n",
    "    plt.imshow(mpimg.imread('results/p2_spectral_distance_v_weights_delta.png'))\n",
    "    plt.axis('off')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('results/p2_adjacency_mean_v_loss.png'):\n",
    "    # Plot mean adjacency versus final training loss for each model in models_list\n",
    "    \n",
    "    # Calculate mean adjacency for each model\n",
    "    adjacency_means = []\n",
    "    for model in models_list:\n",
    "        adjacency_means.append(np.mean(get_adjacency_matrix(model.history['latent'][-1])))\n",
    "    \n",
    "    # Get final training loss for each model\n",
    "    final_losses = []\n",
    "    for model in models_list:\n",
    "        final_losses.append(model.history['train']['loss'][-1])\n",
    "\n",
    "    # Plot mean adjacency versus final training loss\n",
    "    plt.scatter(adjacency_means, final_losses)\n",
    "    plt.xlabel('Adjacency mean')\n",
    "    plt.ylabel('Final training loss')\n",
    "    plt.title('Mean adjacency versus final training loss')\n",
    "\n",
    "    # Save plot to file\n",
    "    plt.savefig(f'results/p2_adjacency_mean_v_loss.png', bbox_inches='tight')\n",
    "    plt.show();\n",
    "else:\n",
    "    # Load plot from file\n",
    "    plt.axes([0,0,1,1])\n",
    "    plt.imshow(mpimg.imread('results/p2_adjacency_mean_v_loss.png'))\n",
    "    plt.axis('off')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Full random search of initial weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "sizes = [784, 30, 2, 10]\n",
    "epochs = 24\n",
    "batch_size = 10\n",
    "learning_rate = 1.5\n",
    "validation_size = 0.3\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Initialize biases array of shape (num_neurons_i, 1) for each layer i except the input layer\n",
    "biases = [np.random.randn(y, 1) for y in sizes[1:]] \n",
    "\n",
    "# Initialize list of data indices for data shuffling per epoch\n",
    "shuffle_order = [np.random.permutation(int(len(train_data)*(1 - validation_size))) for i in range(epochs)]\n",
    "\n",
    "if not os.path.exists('results/p3_models_list.pkl'):\n",
    "    # Train 100 models with different initial weights\n",
    "    models_list = []\n",
    "    for idx, i in enumerate(range(100)):\n",
    "        # Initialize weights array of shape (num_neurons_i, num_neurons_i-1) for each layer i except the input layer\n",
    "        weights = [np.random.randn(y, x) for y, x in zip(sizes[1:], sizes[:-1])]\n",
    "        \n",
    "        # Build & train model\n",
    "        print(f'Training model {idx}...')\n",
    "        model = Model(sizes, weights, biases)\n",
    "        model.fit(train_data, epochs = epochs, batch_size = batch_size, learning_rate = learning_rate, validation_size = validation_size, shuffle_order = shuffle_order, store_latent_vecs = True, verbose=0)\n",
    "        models_list.append(deepcopy(model)) # Deepcopy to avoid overwriting previous models\n",
    "\n",
    "    # Save models_list to file\n",
    "    with open(f'results/p3_models_list.pkl', 'wb') as f:\n",
    "        pickle.dump(models_list, f)\n",
    "else:\n",
    "    # Open models_list from file\n",
    "    with open(f'results/p3_models_list.pkl', 'rb') as f:\n",
    "        models_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('results/p3_latent_over_initweights.png'):\n",
    "    # Visualize final latent space for each model\n",
    "    nrows = 2\n",
    "    ncols = 5\n",
    "    fig, ax = plt.subplots(nrows, ncols, tight_layout=True, figsize = (ncols * 5, nrows * 5))\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    colors = sns.color_palette(['#e6194B','#f58231','#ffe119','#bfef45','#3cb44b','#42d4f4','#4363d8', '#000075', '#911eb4','#f032e6'])\n",
    "\n",
    "    for i, model in enumerate(models_list):\n",
    "        # Get latent vecs for final epoch\n",
    "        latent_vecs = model.history['latent'][-1]\n",
    "        # Create dataframe\n",
    "        df = []\n",
    "        for index, element in enumerate(latent_vecs):\n",
    "            vec, label = element\n",
    "            df.append({'x': vec[0][0], 'y': vec[1][0], 'digit': label})\n",
    "        df = pd.DataFrame(df)\n",
    "\n",
    "        # Plot latent vectors in 2D, color-coded by digit\n",
    "        # print(f'Plotting {len(df)} examples.')\n",
    "        sns.scatterplot(data=df, x=\"x\", y=\"y\", hue=\"digit\", palette=colors, ax=ax[i], legend=False)\n",
    "\n",
    "        # Add title\n",
    "        title = f'Validation accuracy: {model.history[\"val\"][\"acc\"][-1]:.2f}'\n",
    "        ax[i].set_title(title)\n",
    "\n",
    "    # Create legend in last subplot\n",
    "    for i in range(10):\n",
    "        ax[-1].scatter([], [], color=colors[i], label=i)\n",
    "    ax[-1].legend()\n",
    "\n",
    "    # Save plot to file\n",
    "    plt.savefig(f'results/p3_latent_over_initweights.png');\n",
    "    plt.show();\n",
    "else:\n",
    "    # Load plot from file\n",
    "    plt.axes([0,0,1,1])\n",
    "    plt.imshow(mpimg.imread('results/p3_latent_over_initweights.png'))\n",
    "    plt.axis('off')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('results/p3_acc_over_initweights.png'):\n",
    "    # Plot histogram of final training accuracy\n",
    "    plt.hist([model.history['train']['acc'][-1] for model in models_list], bins=10)\n",
    "    plt.xlabel('Final training accuracy')\n",
    "    plt.ylabel('Number of models')\n",
    "    plt.title(f'Variation in training accuracy after {len(models_list[0].history[\"train\"][\"loss\"]) - 1} epochs')\n",
    "    plt.savefig(f'results/p3_acc_over_initweights.png');\n",
    "    plt.show();\n",
    "else:\n",
    "    # Load plot from file\n",
    "    plt.axes([0,0,1,1])\n",
    "    plt.imshow(mpimg.imread('results/p3_acc_over_initweights.png'))\n",
    "    plt.axis('off')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('results/p3_perf_over_initweights.png'):\n",
    "    # Plot training history for each model\n",
    "    nrows = 2\n",
    "    ncols = 5\n",
    "    fig, ax = plt.subplots(nrows, ncols, tight_layout=True, figsize = (ncols * 5, nrows * 5))\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    for i, model in enumerate(models_list):\n",
    "        # Plot accuracy history\n",
    "        ax[i].plot(range(len(model.history['val']['acc'])), model.history['val']['acc'], label='Validation')\n",
    "        ax[i].plot(range(len(model.history['train']['acc'])), model.history['train']['acc'], label='Train')\n",
    "        ax[i].set_xticks(np.arange(0, len(model.history['val']['acc']), 2)) # Set xticks\n",
    "        ax[i].set_ylabel('Accuracy') # Y axis label\n",
    "        ax[i].set_xlabel('Epochs') # X axis label\n",
    "        ax[i].legend() # Add legend\n",
    "\n",
    "        # Add title\n",
    "        title = f'Randomly initialized model {i}'\n",
    "        ax[i].set_title(title)\n",
    "\n",
    "    # Save plot to file\n",
    "    plt.savefig(f'results/p3_perf_over_initweights.png');\n",
    "    plt.show();\n",
    "else:\n",
    "    # Load plot from file\n",
    "    plt.axes([0,0,1,1])\n",
    "    plt.imshow(mpimg.imread('results/p3_perf_over_initweights.png'))\n",
    "    plt.axis('off')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('results/p3_initweights_dist_v_spectral_dist.png'):\n",
    "    # Plot Frobenius distance of first hidden layer initial weights versus spectral distance for each pair of models\n",
    "    frobenius_dists = []\n",
    "    spectral_dists = []    \n",
    "    for i, model1 in enumerate(models_list):\n",
    "        for j, model2 in enumerate(models_list):\n",
    "            if i != j:\n",
    "                # Get first hidden layer initial weights for each model\n",
    "                init_weights_i = model1.initial_weights[0]\n",
    "                init_weights_j = model2.initial_weights[0]\n",
    "                # Get Frobenius distance\n",
    "                frobenius_dists.append(np.linalg.norm(init_weights_i - init_weights_j))\n",
    "                # Get spectral distance of last epoch latent vectors\n",
    "                adjacency_matrix_1 = get_adjacency_matrix(model1.history['latent'][-1])\n",
    "                adjacency_matrix_2 = get_adjacency_matrix(model2.history['latent'][-1])\n",
    "                spectral_dists.append(adjacency_spectral_distance(adjacency_matrix_1, adjacency_matrix_2))\n",
    "    \n",
    "    # Plot Frobenius distance vs. spectral distance\n",
    "    plt.scatter(spectral_dists, frobenius_dists)\n",
    "    plt.xlabel('Spectral distance')\n",
    "    plt.ylabel('Frobenius distance')\n",
    "    plt.title(f'Frobenius distance of first hidden layer initial weights vs. spectral distance after {len(model1.history[\"train\"][\"loss\"]) - 1} epochs')\n",
    "\n",
    "    # Save plot to file\n",
    "    plt.savefig(f'results/p3_initweights_dist_v_spectral_dist.png');\n",
    "    plt.show();\n",
    "\n",
    "else:\n",
    "    # Load plot from file\n",
    "    plt.axes([0,0,1,1])\n",
    "    plt.imshow(mpimg.imread('results/p3_initweights_dist_v_spectral_dist.png'))\n",
    "    plt.axis('off')\n",
    "    plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('results/p3_spectral_dist_v_loss_diff.png'):\n",
    "    # Plot spectral distance versus loss difference for each pair of models\n",
    "    spectral_dists = []\n",
    "    loss_diffs = []\n",
    "    for i, model1 in enumerate(models_list):\n",
    "        for j, model2 in enumerate(models_list):\n",
    "            if i < j:\n",
    "                # Get spectral distance of last epoch latent vectors\n",
    "                adjacency_matrix_1 = get_adjacency_matrix(model1.history['latent'][-1])\n",
    "                adjacency_matrix_2 = get_adjacency_matrix(model2.history['latent'][-1])\n",
    "                spectral_dists.append(adjacency_spectral_distance(adjacency_matrix_1, adjacency_matrix_2))\n",
    "                # Get absolute loss difference\n",
    "                loss_diffs.append(abs(model1.history['train']['loss'][-1] - model2.history['train']['loss'][-1]))\n",
    "\n",
    "    # Plot spectral distance vs. loss difference\n",
    "    plt.scatter(spectral_dists, loss_diffs)\n",
    "    plt.xlabel('Spectral distance')\n",
    "    plt.ylabel('Loss difference')\n",
    "    plt.title(f\"Spectral distance vs. loss difference after {len(model1.history['train']['loss']) - 1} epochs\")\n",
    "\n",
    "    # Save plot to file\n",
    "    plt.savefig(f'results/p3_spectral_dist_v_loss_diff.png');\n",
    "    plt.show();\n",
    "else:\n",
    "    # Load plot from file\n",
    "    plt.axes([0,0,1,1])\n",
    "    plt.imshow(mpimg.imread('results/p3_spectral_dist_v_loss_diff.png'))\n",
    "    plt.axis('off')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('results/p3_initweights_dist_v_loss_diff.png'):\n",
    "    # Plot Frobenius distance versus loss difference for each pair of models\n",
    "    frobenius_dists = []\n",
    "    loss_diffs = []\n",
    "    for i, model1 in enumerate(models_list):\n",
    "        for j, model2 in enumerate(models_list):\n",
    "            if i < j:\n",
    "                # Get first hidden layer initial weights for each model\n",
    "                init_weights_i = model1.initial_weights[0]\n",
    "                init_weights_j = model2.initial_weights[0]\n",
    "                # Get Frobenius distance\n",
    "                frobenius_dists.append(np.linalg.norm(init_weights_i - init_weights_j))\n",
    "                # Get absolute loss difference\n",
    "                loss_diffs.append(abs(model1.history['train']['loss'][-1] - model2.history['train']['loss'][-1]))\n",
    "\n",
    "    # Plot Frobenius distance vs. loss difference\n",
    "    plt.scatter(frobenius_dists, loss_diffs)\n",
    "    plt.xlabel('Frobenius distance')\n",
    "    plt.ylabel('Loss difference')\n",
    "    plt.title(f\"Frobenius distance vs. loss difference after {len(model1.history['train']['loss']) - 1} epochs\")\n",
    "\n",
    "    # Save plot to file\n",
    "    plt.savefig(f'results/p3_initweights_dist_v_loss_diff.png');\n",
    "    plt.show();\n",
    "else:\n",
    "    # Load plot from file\n",
    "    plt.axes([0,0,1,1])\n",
    "    plt.imshow(mpimg.imread('results/p3_initweights_dist_v_loss_diff.png'))\n",
    "    plt.axis('off')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('results/p3_adjacency_heatmaps.png'):\n",
    "    # Plot heatmap of adjacency matrix for each model in models_list\n",
    "    nrows = 2\n",
    "    ncols = 5\n",
    "    fig, ax = plt.subplots(nrows, ncols, tight_layout=True, figsize = (ncols * 5, nrows * 5))\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    for i, model in enumerate(models_list):\n",
    "        # Get adjacency matrix\n",
    "        adjacency_matrix = get_adjacency_matrix(model.history['latent'][-1])\n",
    "        # Plot heatmap\n",
    "        ax[i].imshow(adjacency_matrix, cmap='magma')\n",
    "        ax[i].set_title(f'Last epoch training accuracy: {model.history[\"train\"][\"acc\"][-1]:.2f}')\n",
    "        \n",
    "    # Save plot to file\n",
    "    plt.savefig(f'results/p3_adjacency_heatmaps.png');\n",
    "    plt.show();\n",
    "else:\n",
    "    # Load plot from file\n",
    "    plt.axes([0,0,1,1])\n",
    "    plt.imshow(mpimg.imread('results/p3_adjacency_heatmaps.png'))\n",
    "    plt.axis('off')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0d02bebebd0d339a7f7e0cbd8d22cc3c8fca77e19de36675d07210a32ba28402"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('_stochastic_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
