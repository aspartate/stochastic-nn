{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle, gzip\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "def show(image):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    new = image.copy()\n",
    "    if image.shape == 3:                    # Switch R and B channels so it shows up as correctly as R,G,B, if image is 3-channel\n",
    "        new[:,:,0] = image[:,:,2]\n",
    "        new[:,:,2] = image[:,:,0]\n",
    "    plt.imshow(new, cmap = \"gray\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "\n",
    "with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "    train_data, val_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
    "    # train_data: tuple of (x_train, y_train), where x_train.shape = (50000, 784) and y_train.shape = (50000, 1)\n",
    "    # val_data: tuple of (x_val, y_val), where x_val.shape = (10000, 784) and y_val.shape = (10000, 1)\n",
    "    # test_data: tuple of (x_test, y_test), where x_test.shape = (10000, 784) and y_test.shape = (10000, 1)\n",
    "\n",
    "x_train, y_train = train_data\n",
    "x_val, y_val = val_data\n",
    "x_test, y_test = test_data\n",
    "\n",
    "# Combine training and validation data\n",
    "x_train = np.concatenate((x_train, x_val), axis=0)\n",
    "y_train = np.concatenate((y_train, y_val), axis=0)\n",
    "\n",
    "# Reshape x\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train = np.eye(10)[y_train].reshape((-1, 10, 1))\n",
    "y_test = np.eye(10)[y_test].reshape((-1, 10, 1))\n",
    "\n",
    "# Zip data and labels into tuples\n",
    "train_data = list(zip(x_train, y_train))\n",
    "test_data = list(zip(x_test, y_test))\n",
    "\n",
    "# Shuffle training data\n",
    "np.random.shuffle(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructor for neural network\n",
    "# Adapted from Nielsen textbook http://neuralnetworksanddeeplearning.com/chap1.html\n",
    "\n",
    "class Model(object):\n",
    "\n",
    "    # Fully connected neural network with layer i having sizes[i] neurons\n",
    "    def __init__(self, sizes, weights=None, biases=None):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "\n",
    "        # Initialize weights & biases if not provided\n",
    "        if weights is None:\n",
    "            self.weights = [np.random.randn(y, x) for y, x in zip(sizes[1:], sizes[:-1])] # Initialize weights array of shape (num_neurons_i, num_neurons_i-1) for each layer i except the input layer\n",
    "        else:\n",
    "            self.weights = weights\n",
    "        \n",
    "        if biases is None:\n",
    "            self.biases = [np.random.randn(y, 1) for y in sizes[1:]] # Initialize biases array of shape (num_neurons_i, 1) for each layer i except the input layer\n",
    "        else:\n",
    "            self.biases = biases\n",
    "\n",
    "        # Initialize training history\n",
    "        self.history = {'train': {'acc':[], 'loss':[]}, 'val': {'acc':[], 'loss':[]}, 'latent':[], 'bottleneck_weights':[], 'bottleneck_biases':[]}\n",
    "\n",
    "        # Number of parameters\n",
    "        print(f'Number of model parameters: {sum(np.prod(w.shape) for w in self.weights) + sum(np.prod(b.shape) for b in self.biases)}')\n",
    "\n",
    "    # Activation functions\n",
    "    def activation(self, x):\n",
    "        # x is a vector of length num_hidden_neurons generated by hidden layer affine transformation\n",
    "        def sigmoid(x):\n",
    "            # Overflow-safe sigmoid function (https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/)\n",
    "            if x > 0:\n",
    "                return 1 / (1 + np.exp(-x))\n",
    "            else:\n",
    "                return np.exp(x) / (1 + np.exp(x))\n",
    "        return np.array(list(map(sigmoid, x)))\n",
    "\n",
    "    # Derivative of activation function\n",
    "    def activation_der(self, x):\n",
    "        return self.activation(x) * (1 - self.activation(x))\n",
    "\n",
    "    # # Softmax function\n",
    "    # # Overflow-safe softmax function (https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/)\n",
    "    # def softmax(self, x):\n",
    "    #     # x is unnormalized vector of length output_size generated by output layer affine transformation\n",
    "    #     return np.exp(x - x.max()) / np.sum(np.exp(x - x.max()), axis=0)\n",
    "\n",
    "    # Loss function\n",
    "    def loss(self, y_true, y_pred):\n",
    "        return np.mean(((y_true - y_pred)**2)) # mean squared error\n",
    "\n",
    "    # Derivative of loss function\n",
    "    def loss_der(self, y_true, y_pred):\n",
    "        return 2*(y_pred - y_true)\n",
    "\n",
    "    # Backpropagation\n",
    "    # Input x,y is single training example\n",
    "    # Returns (nabla_b, nabla_w), where nabla_b is list of gradients of cost with respect to biases (one for each layer) and nabla_w is list of gradients of cost with respect to weights (one for each layer)\n",
    "    def backprop(self, x, y):\n",
    "        # Initialize lists of gradients for each layer\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        ###### Forward pass, storing weighted inputs (z) and activations for each layer ######\n",
    "        activation = x # Initialize activation with input\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the weighted input z vectors, layer by layer\n",
    "        # Iterate through each layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation) + b # Compute weighted input z\n",
    "            zs.append(z) # Store weighted input z\n",
    "            activation = self.activation(z) # Compute activation\n",
    "            activations.append(activation) # Store activation\n",
    "\n",
    "        ###### Backward pass, computing gradients of cost with respect to biases and weights ######\n",
    "        # Get gradients for output layer\n",
    "        delta = self.loss_der(y, activations[-1]) * self.activation_der(zs[-1]) # Hadamard product of loss gradient and activation derivative for output layer\n",
    "        nabla_b[-1] = delta # Store gradient of cost with respect to biases of last layer\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose()) # Store gradient of cost with respect to weights of last layer\n",
    "        # Iterate through each layer in reverse order, starting from second to last layer\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l] # Retrieve weighted input z for current layer\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * self.activation_der(z) # Compute gradient of cost with respect to weighted input z\n",
    "            nabla_b[-l] = delta # Store gradient of cost with respect to biases of current layer\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose()) # Store gradient of cost with respect to weights of current layer\n",
    "\n",
    "        # Get training performance\n",
    "        acc = int(np.argmax(y) == np.argmax(activations[-1])) # Whether predicted label is equal to true label\n",
    "        loss = self.loss(y, activations[-1]) # Compute loss\n",
    "\n",
    "        return nabla_b, nabla_w, acc, loss\n",
    "\n",
    "    # Model training using SGD\n",
    "    # training_data is a list of tuples (x, y) representing the training inputs and the desired outputs\n",
    "    def fit(self, training_data, epochs = 10, batch_size = 10, learning_rate = 1.5, validation_size=0.3, shuffle_order=None, store_latent_vecs=False, verbose=True):\n",
    "        \n",
    "        # Take last portion of training data as validation data\n",
    "        training_data = training_data[:-int(validation_size*len(training_data))]\n",
    "        val_data = training_data[-int(validation_size*len(training_data)):]\n",
    "\n",
    "        ## Store baseline model characteristics (before training)\n",
    "        # Evaluate model on validation data\n",
    "        val_acc, val_loss = self.evaluate(val_data)\n",
    "        print(f\"Baseline characteristics: Validation accuracy: {val_acc:.4f}. Mean validation loss: {val_loss:.4f}\")\n",
    "        # Update global history\n",
    "        self.history['train']['acc'].append(np.nan)\n",
    "        self.history['train']['loss'].append(np.nan)\n",
    "        self.history['val']['acc'].append(val_acc)\n",
    "        self.history['val']['loss'].append(val_loss)\n",
    "        # Store latent vectors\n",
    "        if store_latent_vecs:\n",
    "            self.history['latent'].append([(self.feedforward(x, layer_num = 2), np.argmax(y)) for x, y in training_data]) # Store latent representation of training data before training\n",
    "        # Store bottleneck layer weights (second to last layer)\n",
    "        self.history['bottleneck_weights'].append(self.weights[-2])\n",
    "        # Store bottleneck layer biases (second to last layer)\n",
    "        self.history['bottleneck_biases'].append(self.biases[-2])\n",
    "\n",
    "        # Iterate through each epoch\n",
    "        print(f'Training on {len(training_data)} examples, validating on {len(val_data)} examples')\n",
    "        for j in range(epochs):\n",
    "            # Initialize epoch training accuracy and loss history\n",
    "            train_acc = []\n",
    "            train_loss = []\n",
    "            # Get time\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Shuffle training data in preparation for SGD batching\n",
    "            if shuffle_order is None:\n",
    "                np.random.shuffle(training_data) # Shuffle training data randomly\n",
    "            else:\n",
    "                training_data = np.array(training_data, dtype=object)[shuffle_order[j]] # Shuffle training data according to order provided\n",
    "            # Construct training batches for SGD\n",
    "            batches = [training_data[k:k+batch_size] for k in range(0, len(training_data), batch_size)]\n",
    "            # Iterate through all batches\n",
    "            for batch in batches:\n",
    "                nabla_b = [np.zeros(b.shape) for b in self.biases] # Initialize list of loss gradients with respect to biases, one gradient for each layer\n",
    "                nabla_w = [np.zeros(w.shape) for w in self.weights] # Initialize list of loss gradients with respect to weights, one gradient for each layer\n",
    "                # Iterate through all training examples in batch\n",
    "                for x, y in batch:\n",
    "                    delta_nabla_b, delta_nabla_w, acc, loss = self.backprop(x, y) # Compute loss gradients for each layer for single training example\n",
    "                    nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)] # Accumulate bias loss gradients over batches for each layer\n",
    "                    nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)] # Accumulate weight loss gradients over batches for each layer\n",
    "                    # Update epoch training accuracy and loss history for single training example\n",
    "                    train_acc.append(acc)\n",
    "                    train_loss.append(loss)\n",
    "                \n",
    "                # Update weights and biases for each layer using average accumulated loss gradients\n",
    "                self.weights = [w-(learning_rate/len(batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
    "                self.biases = [b-(learning_rate/len(batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
    "            \n",
    "            # Compute epoch training accuracy and loss\n",
    "            train_acc, train_loss = np.mean(train_acc), np.mean(train_loss)\n",
    "            # Evaluate model on validation data\n",
    "            val_acc, val_loss = self.evaluate(val_data)\n",
    "            # Get time\n",
    "            end_time = time.time()\n",
    "            # Print epoch number (epochs start at 1)\n",
    "            if verbose:\n",
    "                print(f\"Epoch {j+1} complete. Time taken: {end_time - start_time:.2f} seconds.\")\n",
    "                print(f\"Training accuracy: {train_acc:.4f}. Validation accuracy: {val_acc:.4f}. Mean training loss: {train_loss:.4f}. Mean validation loss: {val_loss:.4f}\")\n",
    "\n",
    "            # Update global history\n",
    "            self.history['train']['acc'].append(train_acc)\n",
    "            self.history['train']['loss'].append(train_loss)\n",
    "            self.history['val']['acc'].append(val_acc)\n",
    "            self.history['val']['loss'].append(val_loss)\n",
    "            # Store latent vectors\n",
    "            if store_latent_vecs:\n",
    "                self.history['latent'].append([(self.feedforward(x, layer_num = 2), np.argmax(y)) for x, y in training_data]) # Store latent representation of training data after each epoch\n",
    "            # Store bottleneck layer weights (second to last layer)\n",
    "            self.history['bottleneck_weights'].append(self.weights[-2])\n",
    "            # Store bottleneck layer biases (second to last layer)\n",
    "\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in test_data]\n",
    "        acc = sum(int(y_pred == y_true) for (y_pred, y_true) in test_results)/len(test_results)\n",
    "        loss = np.mean([self.loss(self.feedforward(x), y) for (x, y) in test_data])\n",
    "\n",
    "        return acc, loss\n",
    "\n",
    "    # Get feedforward activations for given layer (default is last layer)\n",
    "    # Equivalent to Tensorflow get_layer()\n",
    "    def feedforward(self, x, layer_num = None):\n",
    "        # Dynamically update activations for each layer while moving forward through the network, starting with shape (input_size, 1) and ending with shape (output_size, 1)\n",
    "        if layer_num is None:\n",
    "            layer_num = self.num_layers - 1\n",
    "        activations = x\n",
    "        current_layer = 1\n",
    "        while current_layer <= layer_num:\n",
    "            activations = self.activation(np.dot(self.weights[current_layer - 1], activations) + self.biases[current_layer - 1])\n",
    "            current_layer += 1\n",
    "        return activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set global params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "sizes = [784, 30, 2, 10]\n",
    "epochs = 16\n",
    "batch_size = 10\n",
    "learning_rate = 1.5\n",
    "validation_size=0.3\n",
    "\n",
    "# Initialize weights array of shape (num_neurons_i, num_neurons_i-1) for each layer i except the input layer\n",
    "weights = [np.random.randn(y, x) for y, x in zip(sizes[1:], sizes[:-1])]\n",
    "\n",
    "# Initialize biases array of shape (num_neurons_i, 1) for each layer i except the input layer\n",
    "biases = [np.random.randn(y, 1) for y in sizes[1:]] \n",
    "\n",
    "# Initialize list of data indices for data shuffling per epoch\n",
    "shuffle_order = [np.random.permutation(int(len(train_data)*(1 - validation_size))) for i in range(epochs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracies = []\n",
    "# losses = []\n",
    "\n",
    "# for learning_rate_o in [0.5, 1, 1.5, 2, 2.5, 3]:\n",
    "#     model = Model(sizes, weights, biases)\n",
    "#     model.fit(train_data, epochs = epochs, batch_size = batch_size, learning_rate = learning_rate_o, validation_size = validation_size, shuffle_order = shuffle_order, store_latent_vecs = True, verbose = False)\n",
    "#     test_acc, test_loss = model.evaluate(test_data)\n",
    "#     accuracies.append(test_acc)\n",
    "#     losses.append(test_loss)\n",
    "\n",
    "# print(f'Test accuracies: {accuracies}. Mean test losses: {losses}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that model training is reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build & train first model\n",
    "# model1 = Model(sizes, weights, biases)\n",
    "# model1.fit(train_data, epochs = epochs, batch_size = batch_size, learning_rate = learning_rate, validation_size = validation_size, shuffle_order = shuffle_order, store_latent_vecs = False)\n",
    "# test_acc, test_loss = model1.evaluate(test_data)\n",
    "# print(f'Test accuracy: {test_acc}. Mean test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build & train second model\n",
    "# model2 = Model(sizes, weights, biases)\n",
    "# model2.fit(train_data, epochs = epochs, batch_size = batch_size, learning_rate = learning_rate, validation_size = validation_size, shuffle_order = shuffle_order, store_latent_vecs = True)\n",
    "# test_acc, test_loss = model2.evaluate(test_data)\n",
    "# print(f'Test accuracy: {test_acc}. Mean test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Are model weights reproducible?\n",
    "# print(f'Model weights were reproducible: {[np.array_equal(model1.weights[i], model2.weights[i]) for i in range(len(model1.weights))]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize latent space over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build & train model\n",
    "model = Model(sizes, weights, biases)\n",
    "model.fit(train_data, epochs = 12, batch_size = batch_size, learning_rate = learning_rate, validation_size = validation_size, shuffle_order = shuffle_order, store_latent_vecs = True)\n",
    "test_acc, test_loss = model.evaluate(test_data)\n",
    "print(f'Test accuracy: {test_acc}. Mean test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\azhan\\OneDrive - Harvard University\\Documents\\Research\\stochastic-nn\\main.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/azhan/OneDrive%20-%20Harvard%20University/Documents/Research/stochastic-nn/main.ipynb#ch0000010?line=0'>1</a>\u001b[0m fig, ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, figsize \u001b[39m=\u001b[39m (\u001b[39m10\u001b[39m, \u001b[39m5\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/azhan/OneDrive%20-%20Harvard%20University/Documents/Research/stochastic-nn/main.ipynb#ch0000010?line=2'>3</a>\u001b[0m \u001b[39m# Plot accuracy history\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/azhan/OneDrive%20-%20Harvard%20University/Documents/Research/stochastic-nn/main.ipynb#ch0000010?line=3'>4</a>\u001b[0m ax[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mplot(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(model\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39macc\u001b[39m\u001b[39m'\u001b[39m])), model\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39macc\u001b[39m\u001b[39m'\u001b[39m], label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mValidation\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize = (10, 5))\n",
    "\n",
    "# Plot accuracy history\n",
    "ax[0].plot(range(len(model.history['val']['acc'])), model.history['val']['acc'], label='Validation')\n",
    "ax[0].plot(range(len(model.history['train']['acc'])), model.history['train']['acc'], label='Train')\n",
    "ax[0].xticks(np.arange(0, len(model.history['val']['acc']), 1)) # Set xticks\n",
    "ax[0].ylabel('Accuracy') # Y axis label\n",
    "ax[0].xlabel('Epochs') # X axis label\n",
    "ax[0].legend() # Add legend\n",
    "\n",
    "# Plot loss history\n",
    "ax[1].plot(range(len(model.history['val']['loss'])), model.history['val']['loss'], label='Validation')\n",
    "ax[1].plot(range(len(model.history['train']['loss'])), model.history['train']['loss'], label='Train')\n",
    "ax[1].xticks(np.arange(0, len(model.history['val']['loss']), 1)) # Set xticks\n",
    "ax[1].ylabel('Loss') # Y axis label\n",
    "ax[1].xlabel('Epochs') # X axis label\n",
    "ax[1].legend() # Add legend\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve latent vectors from model history and plot them\n",
    "\n",
    "nrows = int(np.ceil(np.sqrt(len(model.history['latent']))))\n",
    "ncols = int(np.ceil(len(model.history['latent']) / nrows))\n",
    "fig, ax = plt.subplots(nrows, ncols, tight_layout=True, figsize = (ncols * 5, nrows * 5))\n",
    "ax = ax.flatten()\n",
    "\n",
    "for epoch in range(0, len(model.history['latent'])):\n",
    "    # Create dataframe\n",
    "    df = []\n",
    "    for index, element in enumerate(model.history['latent'][epoch]):\n",
    "        vec, label = element\n",
    "        df.append({'x': vec[0][0], 'y': vec[1][0], 'digit': label})\n",
    "    df = pd.DataFrame(df)\n",
    "\n",
    "    # Plot latent vectors in 2D, color-coded by digit\n",
    "    # print(f'Plotting {len(df)} examples.')\n",
    "    colors = sns.color_palette(['#e6194B','#f58231','#ffe119','#bfef45','#3cb44b','#42d4f4','#4363d8', '#000075', '#911eb4','#f032e6'])\n",
    "    sns.scatterplot(data=df, x=\"x\", y=\"y\", hue=\"digit\", palette=colors, ax=ax[epoch], legend=False)\n",
    "\n",
    "    # Add title\n",
    "    if epoch == 0:\n",
    "        title = 'Before training'\n",
    "    else:\n",
    "        title = f'After epoch {epoch}'\n",
    "    ax[epoch].set_title(title)\n",
    "\n",
    "# Create legend for each digit\n",
    "for i in range(10):\n",
    "    ax[-1].scatter([], [], color=colors[i], label=i)\n",
    "ax[-1].legend()\n",
    "\n",
    "# Save plot to file\n",
    "plt.savefig(f'results/latent_over_epochs.png')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Inspect last layer activations for training data\n",
    "# last_layer = [model.feedforward(x, layer_num = 3) for x, y in train_data[:1]]\n",
    "# print(last_layer[0].sum())\n",
    "# print(last_layer[0])\n",
    "# print(train_data[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantify latent space similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get adjacency matrix for a given latent distribution\n",
    "def get_adjacency_matrix(latent_distribution):\n",
    "    # Create dataframe\n",
    "    df = []\n",
    "    for index, element in enumerate(latent_distribution):\n",
    "        vec, label = element\n",
    "        df.append({'x': vec[0][0], 'y': vec[1][0], 'digit': label})\n",
    "    df = pd.DataFrame(df)\n",
    "\n",
    "    # Get centroid coordinates\n",
    "    centroids = df.groupby('digit').mean()\n",
    "\n",
    "    # Create adjacency matrix\n",
    "    adjacency_matrix = np.zeros((10, 10))\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            if i == j:\n",
    "                continue\n",
    "            else:\n",
    "                adjacency_matrix[i][j] = np.linalg.norm(centroids.iloc[i] - centroids.iloc[j]) # Euclidean distance (L2 norm)\n",
    "    \n",
    "    return adjacency_matrix\n",
    "\n",
    "# Function to compute adjacency spectral distance\n",
    "def adjacency_spectral_distance(adjacency_matrix_1, adjacency_matrix_2):\n",
    "    # Compute spectral distance\n",
    "    eigvals_1, eigvecs_1 = np.linalg.eig(adjacency_matrix_1)\n",
    "    eigvals_2, eigvecs_2 = np.linalg.eig(adjacency_matrix_2)\n",
    "    spectral_distance = np.linalg.norm(eigvals_1 - eigvals_2)\n",
    "\n",
    "    return spectral_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate adjacency spectral distance between latent distributions of consecutive epochs (starting from epoch 1)\n",
    "spectral_distances = []\n",
    "for i in range(1, len(model.history['latent']) - 1):\n",
    "    adjacency_matrix_1 = get_adjacency_matrix(model.history['latent'][i])\n",
    "    adjacency_matrix_2 = get_adjacency_matrix(model.history['latent'][i + 1])\n",
    "    spectral_distances.append(adjacency_spectral_distance(adjacency_matrix_1, adjacency_matrix_2))\n",
    "\n",
    "# Get change in training loss between consecutive epochs (starting from epoch 1)\n",
    "loss_deltas = []\n",
    "for i in range(1, len(model.history['train']['loss']) - 1):\n",
    "    loss_deltas.append(model.history['train']['loss'][i + 1] - model.history['train']['loss'][i])\n",
    "\n",
    "# Plot spectral distance vs. change in training loss\n",
    "plt.plot(spectral_distances, loss_deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Frobenius distance between bottleneck weight matrices of consecutive epochs\n",
    "weight_distances = []\n",
    "for i in range(len(model.history['bottleneck_weights']) - 1):\n",
    "    weight_distances.append(np.linalg.norm(model.history['bottleneck_weights'][i + 1] - model.history['bottleneck_weights'][i]))\n",
    "\n",
    "# Plot spectral distance vs. Frobenius weight distance\n",
    "plt.plot(spectral_distances, weight_distances)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b6ab37481d214c2e27d6a7ef14db78afde96efbcea964a935fff031969d172ba"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('nenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
