{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle, gzip\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "def show(image):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    new = image.copy()\n",
    "    if image.shape == 3:                    # Switch R and B channels so it shows up as correctly as R,G,B, if image is 3-channel\n",
    "        new[:,:,0] = image[:,:,2]\n",
    "        new[:,:,2] = image[:,:,0]\n",
    "    plt.imshow(new, cmap = \"gray\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "\n",
    "with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "    train_data, val_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
    "    # train_data: tuple of (x_train, y_train), where x_train.shape = (50000, 784) and y_train.shape = (50000, 1)\n",
    "    # val_data: tuple of (x_val, y_val), where x_val.shape = (10000, 784) and y_val.shape = (10000, 1)\n",
    "    # test_data: tuple of (x_test, y_test), where x_test.shape = (10000, 784) and y_test.shape = (10000, 1)\n",
    "\n",
    "x_train, y_train = train_data\n",
    "x_val, y_val = val_data\n",
    "x_test, y_test = test_data\n",
    "\n",
    "# Combine training and validation data\n",
    "x_train = np.concatenate((x_train, x_val), axis=0)\n",
    "y_train = np.concatenate((y_train, y_val), axis=0)\n",
    "\n",
    "# Reshape x\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train = np.eye(10)[y_train].reshape((-1, 10, 1))\n",
    "y_test = np.eye(10)[y_test].reshape((-1, 10, 1))\n",
    "\n",
    "# Zip data and labels into tuples\n",
    "train_data = list(zip(x_train, y_train))\n",
    "test_data = list(zip(x_test, y_test))\n",
    "\n",
    "# Shuffle training data\n",
    "np.random.shuffle(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructor for neural network\n",
    "# Adapted from Nielsen textbook http://neuralnetworksanddeeplearning.com/chap1.html\n",
    "\n",
    "class Model(object):\n",
    "\n",
    "    # Fully connected neural network with layer i having sizes[i] neurons\n",
    "    def __init__(self, sizes, weights=None, biases=None):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "\n",
    "        # Initialize weights & biases if not provided\n",
    "        if weights is None:\n",
    "            self.weights = [np.random.randn(y, x) for y, x in zip(sizes[1:], sizes[:-1])] # Initialize weights array of shape (num_neurons_i, num_neurons_i-1) for each layer i except the input layer\n",
    "        else:\n",
    "            self.weights = weights\n",
    "        \n",
    "        if biases is None:\n",
    "            self.biases = [np.random.randn(y, 1) for y in sizes[1:]] # Initialize biases array of shape (num_neurons_i, 1) for each layer i except the input layer\n",
    "        else:\n",
    "            self.biases = biases\n",
    "\n",
    "        # Initialize training history\n",
    "        self.history = {'train': {'acc':[], 'loss':[]}, 'val': {'acc':[], 'loss':[]}, 'latent':[]}\n",
    "\n",
    "        # Number of parameters\n",
    "        print(f'Number of model parameters: {sum(np.prod(w.shape) for w in self.weights) + sum(np.prod(b.shape) for b in self.biases)}')\n",
    "\n",
    "    # Activation functions\n",
    "    def activation(self, x):\n",
    "        # x is a vector of length num_hidden_neurons generated by hidden layer affine transformation\n",
    "        def sigmoid(x):\n",
    "            # Overflow-safe sigmoid function (https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/)\n",
    "            if x > 0:\n",
    "                return 1 / (1 + np.exp(-x))\n",
    "            else:\n",
    "                return np.exp(x) / (1 + np.exp(x))\n",
    "        return np.array(list(map(sigmoid, x)))\n",
    "\n",
    "    # Derivative of activation function\n",
    "    def activation_der(self, x):\n",
    "        return self.activation(x) * (1 - self.activation(x))\n",
    "\n",
    "    # # Softmax function\n",
    "    # # Overflow-safe softmax function (https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/)\n",
    "    # def softmax(self, x):\n",
    "    #     # x is unnormalized vector of length output_size generated by output layer affine transformation\n",
    "    #     return np.exp(x - x.max()) / np.sum(np.exp(x - x.max()), axis=0)\n",
    "\n",
    "    # Loss function\n",
    "    def loss(self, y_true, y_pred):\n",
    "        return np.mean(((y_true - y_pred)**2)) # mean squared error\n",
    "\n",
    "    # Derivative of loss function\n",
    "    def loss_der(self, y_true, y_pred):\n",
    "        return 2*(y_pred - y_true)\n",
    "\n",
    "    # Backpropagation\n",
    "    # Input x,y is single training example\n",
    "    # Returns (nabla_b, nabla_w), where nabla_b is list of gradients of cost with respect to biases (one for each layer) and nabla_w is list of gradients of cost with respect to weights (one for each layer)\n",
    "    def backprop(self, x, y):\n",
    "        # Initialize lists of gradients for each layer\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        ###### Forward pass, storing weighted inputs (z) and activations for each layer ######\n",
    "        activation = x # Initialize activation with input\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the weighted input z vectors, layer by layer\n",
    "        # Iterate through each layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation) + b # Compute weighted input z\n",
    "            zs.append(z) # Store weighted input z\n",
    "            activation = self.activation(z) # Compute activation\n",
    "            activations.append(activation) # Store activation\n",
    "\n",
    "        ###### Backward pass ######\n",
    "        # Get gradients for output layer\n",
    "        delta = self.loss_der(y, activations[-1]) * self.activation_der(zs[-1]) # Hadamard product of loss gradient and activation derivative\n",
    "        nabla_b[-1] = delta # Store gradient of cost with respect to biases of last layer\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose()) # Store gradient of cost with respect to weights of last layer\n",
    "        # Iterate through each layer in reverse order, starting from second to last layer\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l] # Retrieve weighted input z for current layer\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * self.activation_der(z) # Compute gradient of cost with respect to weighted input z\n",
    "            nabla_b[-l] = delta # Store gradient of cost with respect to biases of current layer\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose()) # Store gradient of cost with respect to weights of current layer\n",
    "\n",
    "        # Get training performance\n",
    "        acc = int(np.argmax(y) == np.argmax(activations[-1])) # Whether predicted label is equal to true label\n",
    "        loss = self.loss(y, activations[-1]) # Compute loss\n",
    "\n",
    "        return nabla_b, nabla_w, acc, loss\n",
    "\n",
    "    # Model training using SGD\n",
    "    # training_data is a list of tuples (x, y) representing the training inputs and the desired outputs\n",
    "    def fit(self, training_data, epochs = 10, batch_size = 10, learning_rate = 1.5, validation_size=0.3, shuffle_order=None, store_latent_vecs=False):\n",
    "        \n",
    "        # Take last portion of training data as validation data\n",
    "        training_data = training_data[:-int(validation_size*len(training_data))]\n",
    "        val_data = training_data[-int(validation_size*len(training_data)):]\n",
    "\n",
    "        ## Store baseline model characteristics (before training)\n",
    "        # Evaluate model on validation data\n",
    "        val_acc, val_loss = self.evaluate(val_data)\n",
    "        print(f\"Baseline characteristics: Validation accuracy: {val_acc:.4f}. Mean validation loss: {val_loss:.4f}\")\n",
    "        # Update global history\n",
    "        self.history['train']['acc'].append(np.nan)\n",
    "        self.history['train']['loss'].append(np.nan)\n",
    "        self.history['val']['acc'].append(val_acc)\n",
    "        self.history['val']['loss'].append(val_loss)\n",
    "        # Store latent vectors\n",
    "        if store_latent_vecs:\n",
    "            self.history['latent'].append([(self.feedforward(x, layer_num = 2), np.argmax(y)) for x, y in training_data]) # Store latent representation of training data before training\n",
    "\n",
    "        # Iterate through each epoch\n",
    "        print(f'Training on {len(training_data)} examples, validating on {len(val_data)} examples')\n",
    "        for j in range(epochs):\n",
    "            # Initialize epoch training accuracy and loss history\n",
    "            train_acc = []\n",
    "            train_loss = []\n",
    "            # Get time\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Shuffle training data in preparation for SGD batching\n",
    "            if shuffle_order is None:\n",
    "                np.random.shuffle(training_data) # Shuffle training data randomly\n",
    "            else:\n",
    "                training_data = np.array(training_data, dtype=object)[shuffle_order[j]] # Shuffle training data according to order provided\n",
    "            # Construct training batches for SGD\n",
    "            batches = [training_data[k:k+batch_size] for k in range(0, len(training_data), batch_size)]\n",
    "            # Iterate through all batches\n",
    "            for batch in batches:\n",
    "                nabla_b = [np.zeros(b.shape) for b in self.biases] # Initialize list of loss gradients with respect to biases, one gradient for each layer\n",
    "                nabla_w = [np.zeros(w.shape) for w in self.weights] # Initialize list of loss gradients with respect to weights, one gradient for each layer\n",
    "                # Iterate through all training examples in batch\n",
    "                for x, y in batch:\n",
    "                    delta_nabla_b, delta_nabla_w, acc, loss = self.backprop(x, y) # Compute loss gradients for each layer for single training example\n",
    "                    nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)] # Accumulate bias loss gradients over batches for each layer\n",
    "                    nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)] # Accumulate weight loss gradients over batches for each layer\n",
    "                    # Update epoch training accuracy and loss history for single training example\n",
    "                    train_acc.append(acc)\n",
    "                    train_loss.append(loss)\n",
    "                \n",
    "                # Update weights and biases for each layer using average accumulated loss gradients\n",
    "                self.weights = [w-(learning_rate/len(batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
    "                self.biases = [b-(learning_rate/len(batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
    "            \n",
    "            # Compute epoch training accuracy and loss\n",
    "            train_acc, train_loss = np.mean(train_acc), np.mean(train_loss)\n",
    "            # Evaluate model on validation data\n",
    "            val_acc, val_loss = self.evaluate(val_data)\n",
    "            # Get time\n",
    "            end_time = time.time()\n",
    "            # Print epoch number (epochs start at 1)\n",
    "            print(f\"Epoch {j+1} complete. Time taken: {end_time - start_time:.2f} seconds.\")\n",
    "            print(f\"Training accuracy: {train_acc:.4f}. Validation accuracy: {val_acc:.4f}. Mean training loss: {train_loss:.4f}. Mean validation loss: {val_loss:.4f}\")\n",
    "\n",
    "            # Update global history\n",
    "            self.history['train']['acc'].append(train_acc)\n",
    "            self.history['train']['loss'].append(train_loss)\n",
    "            self.history['val']['acc'].append(val_acc)\n",
    "            self.history['val']['loss'].append(val_loss)\n",
    "            # Store latent vectors\n",
    "            if store_latent_vecs:\n",
    "                self.history['latent'].append([(self.feedforward(x, layer_num = 2), np.argmax(y)) for x, y in training_data]) # Store latent representation of training data after each epoch\n",
    "\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in test_data]\n",
    "        acc = sum(int(y_pred == y_true) for (y_pred, y_true) in test_results)/len(test_results)\n",
    "        loss = np.mean([self.loss(self.feedforward(x), y) for (x, y) in test_data])\n",
    "\n",
    "        return acc, loss\n",
    "\n",
    "    # Get feedforward activations for given layer (default is last layer)\n",
    "    # Equivalent to Tensorflow get_layer()\n",
    "    def feedforward(self, x, layer_num = None):\n",
    "        # Dynamically update activations for each layer while moving forward through the network, starting with shape (input_size, 1) and ending with shape (output_size, 1)\n",
    "        if layer_num is None:\n",
    "            layer_num = self.num_layers - 1\n",
    "        activations = x\n",
    "        current_layer = 1\n",
    "        while current_layer <= layer_num:\n",
    "            activations = self.activation(np.dot(self.weights[current_layer - 1], activations) + self.biases[current_layer - 1])\n",
    "            current_layer += 1\n",
    "        return activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set global stochastic params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "sizes = [784, 30, 2, 10]\n",
    "epochs = 10\n",
    "batch_size = 10\n",
    "learning_rate = 1.5\n",
    "validation_size=0.3\n",
    "\n",
    "# Initialize weights array of shape (num_neurons_i, num_neurons_i-1) for each layer i except the input layer\n",
    "weights = [np.random.randn(y, x) for y, x in zip(sizes[1:], sizes[:-1])]\n",
    "\n",
    "# Initialize biases array of shape (num_neurons_i, 1) for each layer i except the input layer\n",
    "biases = [np.random.randn(y, 1) for y in sizes[1:]] \n",
    "\n",
    "# Initialize list of data indices for data shuffling per epoch\n",
    "shuffle_order = [np.random.permutation(int(len(train_data)*(1 - validation_size))) for i in range(epochs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that model training is reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model parameters: 23642\n",
      "Baseline characteristics: Validation accuracy: 0.09619047619047619. Mean validation loss: 0.3913636964617906\n",
      "Training on 42000 examples, validating on 12600 examples\n",
      "Epoch 1 complete. Time taken: 66.02 seconds.\n",
      "Training accuracy: 0.40576190476190477. Validation accuracy: 0.5238095238095238. Mean training loss: 0.06967855704023915. Mean validation loss: 0.061993250668054396\n",
      "Epoch 2 complete. Time taken: 68.37 seconds.\n",
      "Training accuracy: 0.5539285714285714. Validation accuracy: 0.6136507936507937. Mean training loss: 0.0592155730971208. Mean validation loss: 0.05632097621720824\n",
      "Epoch 3 complete. Time taken: 66.92 seconds.\n",
      "Training accuracy: 0.6072619047619048. Validation accuracy: 0.6633333333333333. Mean training loss: 0.05494025965816666. Mean validation loss: 0.05160415026532875\n",
      "Epoch 4 complete. Time taken: 67.30 seconds.\n",
      "Training accuracy: 0.6270238095238095. Validation accuracy: 0.6349206349206349. Mean training loss: 0.051984795721949806. Mean validation loss: 0.05158323776861175\n",
      "Epoch 5 complete. Time taken: 65.70 seconds.\n",
      "Training accuracy: 0.649547619047619. Validation accuracy: 0.6631746031746032. Mean training loss: 0.05050333198588217. Mean validation loss: 0.05224608162092851\n",
      "Epoch 6 complete. Time taken: 64.19 seconds.\n",
      "Training accuracy: 0.6703095238095238. Validation accuracy: 0.5176984126984127. Mean training loss: 0.04929977757110336. Mean validation loss: 0.05588446093717504\n",
      "Epoch 7 complete. Time taken: 65.16 seconds.\n",
      "Training accuracy: 0.686. Validation accuracy: 0.6127777777777778. Mean training loss: 0.047910130008575386. Mean validation loss: 0.04897688373936934\n",
      "Epoch 8 complete. Time taken: 63.75 seconds.\n",
      "Training accuracy: 0.7045. Validation accuracy: 0.7301587301587301. Mean training loss: 0.046158032561517646. Mean validation loss: 0.042847714893868\n",
      "Epoch 9 complete. Time taken: 66.41 seconds.\n",
      "Training accuracy: 0.7237857142857143. Validation accuracy: 0.7610317460317461. Mean training loss: 0.04419261578358143. Mean validation loss: 0.042741395448858334\n",
      "Epoch 10 complete. Time taken: 64.13 seconds.\n",
      "Training accuracy: 0.7322619047619048. Validation accuracy: 0.7508730158730159. Mean training loss: 0.042872316161637065. Mean validation loss: 0.03938495690922791\n",
      "Test accuracy: 0.7483. Mean test loss: 0.04045542785489169\n"
     ]
    }
   ],
   "source": [
    "# Build & train first model\n",
    "model1 = Model(sizes, weights, biases)\n",
    "model1.fit(train_data, epochs = epochs, batch_size = batch_size, learning_rate = learning_rate, validation_size = validation_size, shuffle_order = shuffle_order, store_latent_vecs = False)\n",
    "test_acc, test_loss = model1.evaluate(test_data)\n",
    "print(f'Test accuracy: {test_acc}. Mean test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model parameters: 23642\n",
      "Baseline characteristics: Validation accuracy: 0.09619047619047619. Mean validation loss: 0.3913636964617906\n",
      "Training on 42000 examples, validating on 12600 examples\n",
      "Epoch 1 complete. Time taken: 60.10 seconds.\n",
      "Training accuracy: 0.40576190476190477. Validation accuracy: 0.5238095238095238. Mean training loss: 0.06967855704023915. Mean validation loss: 0.061993250668054396\n",
      "Epoch 2 complete. Time taken: 55.77 seconds.\n",
      "Training accuracy: 0.5539285714285714. Validation accuracy: 0.6136507936507937. Mean training loss: 0.0592155730971208. Mean validation loss: 0.05632097621720824\n",
      "Epoch 3 complete. Time taken: 55.37 seconds.\n",
      "Training accuracy: 0.6072619047619048. Validation accuracy: 0.6633333333333333. Mean training loss: 0.05494025965816666. Mean validation loss: 0.05160415026532875\n",
      "Epoch 4 complete. Time taken: 54.85 seconds.\n",
      "Training accuracy: 0.6270238095238095. Validation accuracy: 0.6349206349206349. Mean training loss: 0.051984795721949806. Mean validation loss: 0.05158323776861175\n",
      "Epoch 5 complete. Time taken: 54.61 seconds.\n",
      "Training accuracy: 0.649547619047619. Validation accuracy: 0.6631746031746032. Mean training loss: 0.05050333198588217. Mean validation loss: 0.05224608162092851\n",
      "Epoch 6 complete. Time taken: 54.83 seconds.\n",
      "Training accuracy: 0.6703095238095238. Validation accuracy: 0.5176984126984127. Mean training loss: 0.04929977757110336. Mean validation loss: 0.05588446093717504\n",
      "Epoch 7 complete. Time taken: 55.34 seconds.\n",
      "Training accuracy: 0.686. Validation accuracy: 0.6127777777777778. Mean training loss: 0.047910130008575386. Mean validation loss: 0.04897688373936934\n",
      "Epoch 8 complete. Time taken: 56.39 seconds.\n",
      "Training accuracy: 0.7045. Validation accuracy: 0.7301587301587301. Mean training loss: 0.046158032561517646. Mean validation loss: 0.042847714893868\n",
      "Epoch 9 complete. Time taken: 57.96 seconds.\n",
      "Training accuracy: 0.7237857142857143. Validation accuracy: 0.7610317460317461. Mean training loss: 0.04419261578358143. Mean validation loss: 0.042741395448858334\n",
      "Epoch 10 complete. Time taken: 58.56 seconds.\n",
      "Training accuracy: 0.7322619047619048. Validation accuracy: 0.7508730158730159. Mean training loss: 0.042872316161637065. Mean validation loss: 0.03938495690922791\n",
      "Test accuracy: 0.7483. Mean test loss: 0.04045542785489169\n"
     ]
    }
   ],
   "source": [
    "# Build & train second model\n",
    "model2 = Model(sizes, weights, biases)\n",
    "model2.fit(train_data, epochs = epochs, batch_size = batch_size, learning_rate = learning_rate, validation_size = validation_size, shuffle_order = shuffle_order, store_latent_vecs = True)\n",
    "test_acc, test_loss = model2.evaluate(test_data)\n",
    "print(f'Test accuracy: {test_acc}. Mean test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights were reproducible: [True, True, True]\n"
     ]
    }
   ],
   "source": [
    "print(f'Model weights were reproducible: {[np.array_equal(model1.weights[i], model2.weights[i]) for i in range(len(model1.weights))]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build & train model\n",
    "model2 = Model([784, 28, 10, 2, 10])\n",
    "model2.fit(train_data, epochs = 10, batch_size = 10, learning_rate = 1.5, validation_size = 0.3)\n",
    "test_acc, test_loss = model2.evaluate(test_data)\n",
    "print(f'Test accuracy: {test_acc}. Mean test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build & train model\n",
    "model3 = Model([784, 28, 28, 2, 10])\n",
    "model3.fit(train_data, epochs = 10, batch_size = 10, learning_rate = 1.5, validation_size = 0.3)\n",
    "test_acc, test_loss = model3.evaluate(test_data)\n",
    "print(f'Test accuracy: {test_acc}. Mean test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build & train model\n",
    "model4 = Model([784, 28, 2, 10])\n",
    "model4.fit(train_data, epochs = 10, batch_size = 10, learning_rate = 1.5, validation_size = 0.3)\n",
    "test_acc, test_loss = model4.evaluate(test_data)\n",
    "print(f'Test accuracy: {test_acc}. Mean test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build & train model\n",
    "model5 = Model([784, 28, 2, 10])\n",
    "model5.fit(train_data, epochs = 30, batch_size = 10, learning_rate = 1, validation_size = 0.3)\n",
    "test_acc, test_loss = model5.evaluate(test_data)\n",
    "print(f'Test accuracy: {test_acc}. Mean test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build & train model\n",
    "model6 = Model([784, 28, 10, 2, 10])\n",
    "model6.fit(train_data, epochs = 10, batch_size = 10, learning_rate = 1.5, validation_size = 0.3)\n",
    "test_acc, test_loss = model6.evaluate(test_data)\n",
    "print(f'Test accuracy: {test_acc}. Mean test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy history\n",
    "plt.plot(range(len(model1.history['val']['acc'])), model1.history['val']['acc'], label='Model 1')\n",
    "plt.plot(range(len(model2.history['val']['acc'])), model2.history['val']['acc'], label='Model 2')\n",
    "plt.plot(range(len(model3.history['val']['acc'])), model3.history['val']['acc'], label='Model 3')\n",
    "plt.plot(range(len(model4.history['val']['acc'])), model4.history['val']['acc'], label='Model 4')\n",
    "plt.plot(range(len(model5.history['val']['acc'])), model5.history['val']['acc'], label='Model 5')\n",
    "plt.plot(range(len(model6.history['val']['acc'])), model6.history['val']['acc'], label='Model 6')\n",
    "plt.xticks(np.arange(0, len(model1.history['val']['acc']), 1)) # Set xticks\n",
    "plt.ylabel('Accuracy') # Y axis label\n",
    "plt.xlabel('Epochs') # X axis label\n",
    "plt.legend() # Add legend\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot accuracy history\n",
    "# plt.plot(range(len(model.history['val']['acc'])), model.history['val']['acc'], label='Validation')\n",
    "# plt.plot(range(len(model.history['train']['acc'])), model.history['train']['acc'], label='Train')\n",
    "# plt.xticks(np.arange(0, len(model.history['val']['acc']), 1)) # Set xticks\n",
    "# plt.ylabel('Accuracy') # Y axis label\n",
    "# plt.xlabel('Epochs') # X axis label\n",
    "# plt.legend() # Add legend\n",
    "# plt.show();\n",
    "\n",
    "# # Plot loss history\n",
    "# plt.plot(range(len(model.history['val']['loss'])), model.history['val']['loss'], label='Validation')\n",
    "# plt.plot(range(len(model.history['train']['loss'])), model.history['train']['loss'], label='Train')\n",
    "# plt.xticks(np.arange(0, len(model.history['val']['loss']), 1)) # Set xticks\n",
    "# plt.ylabel('Loss') # Y axis label\n",
    "# plt.xlabel('Epochs') # X axis label\n",
    "# plt.legend() # Add legend\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get list of layer activations (latent vectors) for bottleneck layer for training data\n",
    "# latent_vecs = [model.feedforward(x, layer_num = 3) for x, y in train_data]\n",
    "\n",
    "# # Create dataframe\n",
    "# df = []\n",
    "# for index, vec in enumerate(latent_vecs):\n",
    "#     df.append({'x': vec[0][0], 'y': vec[1][0], 'digit': np.argmax(train_data[index][1])})\n",
    "# df = pd.DataFrame(df)\n",
    "\n",
    "# # Plot latent vectors in 2D, color-coded by digit\n",
    "# print(f'Plotting {len(df)} examples.')\n",
    "# colors = sns.color_palette(['#e6194B','#f58231','#ffe119','#bfef45','#3cb44b','#42d4f4','#4363d8', '#000075', '#911eb4','#f032e6'])\n",
    "# sns.scatterplot(data=df, x=\"x\", y=\"y\", hue=\"digit\", palette=colors);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreive latent vectors from model history\n",
    "\n",
    "for epoch in range(0, len(model4.history['latent'])):\n",
    "    # Create dataframe\n",
    "    df = []\n",
    "    for index, element in enumerate(model4.history['latent'][epoch]):\n",
    "        vec, label = element\n",
    "        df.append({'x': vec[0][0], 'y': vec[1][0], 'digit': label})\n",
    "    df = pd.DataFrame(df)\n",
    "\n",
    "    # Plot latent vectors in 2D, color-coded by digit\n",
    "    print(f'Plotting {len(df)} examples.')\n",
    "    colors = sns.color_palette(['#e6194B','#f58231','#ffe119','#bfef45','#3cb44b','#42d4f4','#4363d8', '#000075', '#911eb4','#f032e6'])\n",
    "    sns.scatterplot(data=df, x=\"x\", y=\"y\", hue=\"digit\", palette=colors);\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n",
    "    # Save plot to file\n",
    "    plt.savefig(f'epoch_{epoch}.png')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Inspect last layer activations for training data\n",
    "# last_layer = [model.feedforward(x, layer_num = 4) for x, y in train_data[:1]]\n",
    "# print(last_layer[0].sum())\n",
    "# print(last_layer[0])\n",
    "# print(train_data[0][1])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b6ab37481d214c2e27d6a7ef14db78afde96efbcea964a935fff031969d172ba"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('nenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
